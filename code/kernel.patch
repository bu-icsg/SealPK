diff --git a/arch/riscv/Kconfig b/arch/riscv/Kconfig
index e716922..1628a4f 100644
--- a/arch/riscv/Kconfig
+++ b/arch/riscv/Kconfig
@@ -345,6 +345,19 @@ config SECCOMP
 
 	  If unsure, say Y. Only embedded should say N here.
 
+config RISCV_MEMORY_PROTECTION_KEYS
+	prompt "RISC-V Memory Protection Keys"
+	def_bool y
+	depends on 64BIT
+	select ARCH_USES_HIGH_VMA_FLAGS
+	select ARCH_HAS_PKEYS
+	---help---
+	  Memory Protection Keys provides a mechanism for enforcing
+	  page-based protections, but without requiring modification of the
+	  page tables when an application changes protection domains.
+
+	  If unsure, say y.
+    
 config EARLY_PRINTK
 	def_bool y
 
diff --git a/arch/riscv/include/asm/irq.h b/arch/riscv/include/asm/irq.h
index 4dee9d4..cd8e713 100644
--- a/arch/riscv/include/asm/irq.h
+++ b/arch/riscv/include/asm/irq.h
@@ -20,6 +20,9 @@
 #define INTERRUPT_CAUSE_SOFTWARE    1
 #define INTERRUPT_CAUSE_TIMER       5
 #define INTERRUPT_CAUSE_EXTERNAL    9
+/* PHMon: begin */
+#define INTERRUPT_CAUSE_ROCC        12
+/* PHMon: end */
 
 void riscv_timer_interrupt(void);
 
diff --git a/arch/riscv/include/asm/mmu.h b/arch/riscv/include/asm/mmu.h
index 5df2dcc..e565f1f 100644
--- a/arch/riscv/include/asm/mmu.h
+++ b/arch/riscv/include/asm/mmu.h
@@ -23,6 +23,21 @@ typedef struct {
 	/* A local icache flush is needed before user execution can resume. */
 	cpumask_t icache_stale_mask;
 #endif
+
+/* PHMon: begin */
+#ifdef CONFIG_RISCV_MEMORY_PROTECTION_KEYS
+	/*
+	 * One bit per protection key says whether userspace can
+	 * use it or not.  protected by mmap_sem.
+	 */
+	pkey_umap_t pkey_allocation_map;
+    pkey_dmap_t pkey_dirty_map;
+    s16 execute_only_pkey;
+    pkey_seal_t pkey_domain_seal_map;
+    pkey_seal_t pkey_page_seal_map;
+#endif
+/* PHMon: end */
+  
 } mm_context_t;
 
 #endif /* __ASSEMBLY__ */
diff --git a/arch/riscv/include/asm/mmu_context.h b/arch/riscv/include/asm/mmu_context.h
index 336d60e..0a06b23 100644
--- a/arch/riscv/include/asm/mmu_context.h
+++ b/arch/riscv/include/asm/mmu_context.h
@@ -16,12 +16,18 @@
 #define _ASM_RISCV_MMU_CONTEXT_H
 
 #include <linux/mm_types.h>
-#include <asm-generic/mm_hooks.h>
+/* PHMon: begin */
+//#include <asm-generic/mm_hooks.h>
+/* PHMon: end */
 
 #include <linux/mm.h>
 #include <linux/sched.h>
 #include <asm/tlbflush.h>
 #include <asm/cacheflush.h>
+/* PHMon: begin */
+#ifdef CONFIG_RISCV_MEMORY_PROTECTION_KEYS
+#endif
+/* PHMon: end */
 
 static inline void enter_lazy_tlb(struct mm_struct *mm,
 	struct task_struct *task)
@@ -32,6 +38,15 @@ static inline void enter_lazy_tlb(struct mm_struct *mm,
 static inline int init_new_context(struct task_struct *task,
 	struct mm_struct *mm)
 {
+    /* PHMon: begin */
+    #ifdef CONFIG_RISCV_MEMORY_PROTECTION_KEYS
+      /* pkey 0 is the default and always allocated */
+      mm->context.pkey_allocation_map.row[0] = 0x1;
+      /* -1 means unallocated or invalid */
+      mm->context.execute_only_pkey = -1;
+      /* by default no pkey is dirty */
+    #endif
+    /* PHMon: end */
 	return 0;
 }
 
@@ -106,4 +121,95 @@ static inline void deactivate_mm(struct task_struct *task,
 {
 }
 
+/* PHMon: begin */
+#ifdef CONFIG_RISCV_MEMORY_PROTECTION_KEYS
+static inline int vma_pkey(struct vm_area_struct *vma)
+{
+	unsigned long vma_pkey_mask = VM_PKEY_BIT0 | VM_PKEY_BIT1 |
+				      VM_PKEY_BIT2 | VM_PKEY_BIT3 | VM_PKEY_BIT4 |
+                      VM_PKEY_BIT4 | VM_PKEY_BIT5 | VM_PKEY_BIT6 |
+                      VM_PKEY_BIT7 | VM_PKEY_BIT8 | VM_PKEY_BIT9;
+    /* PHMon: begin */
+    #ifdef PHMON_DEBUG
+    printk("vma_pkey ---- vma_pkey_mask: 0x%lx shift: 0x%x vma_pkey: 0x%lx\n", vma_pkey_mask, VM_PKEY_SHIFT, ((vma->vm_flags & vma_pkey_mask) >> VM_PKEY_SHIFT));
+    #endif
+    /* PHMon: end */
+	return (vma->vm_flags & vma_pkey_mask) >> VM_PKEY_SHIFT;
+}
+#else
+static inline int vma_pkey(struct vm_area_struct *vma)
+{
+	return 0;
+}
+#endif
+
+/*
+ * We only want to enforce protection keys on the current process
+ * because we effectively have no access to PKRU for other
+ * processes or any way to tell *which * PKRU in a threaded
+ * process we could use.
+ *
+ * So do not enforce things if the VMA is not from the current
+ * mm, or if we are in a kernel thread.
+ */
+static inline bool vma_is_foreign(struct vm_area_struct *vma)
+{
+	if (!current->mm)
+		return true;
+	/*
+	 * Should PKRU be enforced on the access to this VMA?  If
+	 * the VMA is from another process, then PKRU has no
+	 * relevance and should not be enforced.
+	 */
+	if (current->mm != vma->vm_mm)
+		return true;
+
+	return false;
+}
+
+static inline bool arch_vma_access_permitted(struct vm_area_struct *vma,
+		bool write, bool execute, bool foreign)
+{
+    bool result = false;
+	/* pkeys never affect instruction fetches */
+	if (execute)
+		return true;
+	/* allow access if the VMA is not one from this process */
+	if (foreign || vma_is_foreign(vma))
+		return true;
+    #ifdef PHMON_DEBUG
+    printk("arch_vma_access_permitted\n");
+    #endif
+	//return __pkru_allows_pkey(vma_pkey(vma), write);
+    result = __pkru_allows_pkey(vma_pkey(vma), write);
+    #ifdef PHMON_DEBUG
+    printk("arch_vma_access_permitted: restul? %d\n", result);
+    #endif
+    return result;
+}
+
+/* Coppied from <include/asm_generic/mm_hooks.h> to avoid including it */
+static inline int arch_dup_mmap(struct mm_struct *oldmm,
+				struct mm_struct *mm)
+{
+	return 0;
+}
+
+static inline void arch_exit_mmap(struct mm_struct *mm)
+{
+}
+
+static inline void arch_unmap(struct mm_struct *mm,
+			struct vm_area_struct *vma,
+			unsigned long start, unsigned long end)
+{
+}
+
+static inline void arch_bprm_mm_init(struct mm_struct *mm,
+				     struct vm_area_struct *vma)
+{
+}
+
+/* PHMon: end */
+
 #endif /* _ASM_RISCV_MMU_CONTEXT_H */
diff --git a/arch/riscv/include/asm/pgtable-64.h b/arch/riscv/include/asm/pgtable-64.h
index 7aa0ea9..3a9125f 100644
--- a/arch/riscv/include/asm/pgtable-64.h
+++ b/arch/riscv/include/asm/pgtable-64.h
@@ -26,6 +26,15 @@
 #define PMD_SIZE        (_AC(1, UL) << PMD_SHIFT)
 #define PMD_MASK        (~(PMD_SIZE - 1))
 
+/* PHMon: begin */
+typedef unsigned long	pteval_t;
+typedef unsigned long	pmdval_t;
+typedef unsigned long	pudval_t;
+typedef unsigned long	p4dval_t;
+typedef unsigned long	pgdval_t;
+typedef unsigned long	pgprotval_t;
+/* PHMon: end */
+
 /* Page Middle Directory entry */
 typedef struct {
 	unsigned long pmd;
diff --git a/arch/riscv/include/asm/pgtable-bits.h b/arch/riscv/include/asm/pgtable-bits.h
index 997ddbb..690777f 100644
--- a/arch/riscv/include/asm/pgtable-bits.h
+++ b/arch/riscv/include/asm/pgtable-bits.h
@@ -36,6 +36,58 @@
 #define _PAGE_TABLE     _PAGE_PRESENT
 
 #define _PAGE_PFN_SHIFT 10
+/* PHMon: begin */
+#define _PAGE_PD_SIZE 10
+#define _PAGE_PD_SHIFT 54
+#define _PAGE_PFN_R_SHIFT 20
+
+#define _PAGE_BIT_PKEY_BIT0	54	/* Protection Keys, bit 1/10 */
+#define _PAGE_BIT_PKEY_BIT1	55	/* Protection Keys, bit 2/10 */
+#define _PAGE_BIT_PKEY_BIT2	56	/* Protection Keys, bit 3/10 */
+#define _PAGE_BIT_PKEY_BIT3	57	/* Protection Keys, bit 4/10 */
+#define _PAGE_BIT_PKEY_BIT4	58	/* Protection Keys, bit 5/10 */
+#define _PAGE_BIT_PKEY_BIT5	59	/* Protection Keys, bit 6/10 */
+#define _PAGE_BIT_PKEY_BIT6	60	/* Protection Keys, bit 7/10 */
+#define _PAGE_BIT_PKEY_BIT7	61	/* Protection Keys, bit 8/10 */
+#define _PAGE_BIT_PKEY_BIT8	62	/* Protection Keys, bit 9/10 */
+#define _PAGE_BIT_PKEY_BIT9	63	/* Protection Keys, bit 10/10 */
+
+#ifdef CONFIG_RISCV_MEMORY_PROTECTION_KEYS
+#define _PAGE_PKEY_BIT0	(1UL << _PAGE_BIT_PKEY_BIT0)
+#define _PAGE_PKEY_BIT1	(1UL << _PAGE_BIT_PKEY_BIT1)
+#define _PAGE_PKEY_BIT2	(1UL << _PAGE_BIT_PKEY_BIT2)
+#define _PAGE_PKEY_BIT3	(1UL << _PAGE_BIT_PKEY_BIT3)
+#define _PAGE_PKEY_BIT4	(1UL << _PAGE_BIT_PKEY_BIT4)
+#define _PAGE_PKEY_BIT5	(1UL << _PAGE_BIT_PKEY_BIT5)
+#define _PAGE_PKEY_BIT6	(1UL << _PAGE_BIT_PKEY_BIT6)
+#define _PAGE_PKEY_BIT7	(1UL << _PAGE_BIT_PKEY_BIT7)
+#define _PAGE_PKEY_BIT8	(1UL << _PAGE_BIT_PKEY_BIT8)
+#define _PAGE_PKEY_BIT9	(1UL << _PAGE_BIT_PKEY_BIT9)
+#else
+#define _PAGE_PKEY_BIT0	(0UL << _PAGE_BIT_PKEY_BIT0)
+#define _PAGE_PKEY_BIT1	(0UL << _PAGE_BIT_PKEY_BIT1)
+#define _PAGE_PKEY_BIT2	(0UL << _PAGE_BIT_PKEY_BIT2)
+#define _PAGE_PKEY_BIT3	(0UL << _PAGE_BIT_PKEY_BIT3)
+#define _PAGE_PKEY_BIT4	(0UL << _PAGE_BIT_PKEY_BIT4)
+#define _PAGE_PKEY_BIT5	(0UL << _PAGE_BIT_PKEY_BIT5)
+#define _PAGE_PKEY_BIT6	(0UL << _PAGE_BIT_PKEY_BIT6)
+#define _PAGE_PKEY_BIT7	(0UL << _PAGE_BIT_PKEY_BIT7)
+#define _PAGE_PKEY_BIT8	(0UL << _PAGE_BIT_PKEY_BIT8)
+#define _PAGE_PKEY_BIT9	(0UL << _PAGE_BIT_PKEY_BIT9)
+#endif
+
+#define _PAGE_PKEY_MASK (_PAGE_PKEY_BIT0 | \
+			 _PAGE_PKEY_BIT1 | \
+			 _PAGE_PKEY_BIT2 | \
+			 _PAGE_PKEY_BIT3 | \
+			 _PAGE_PKEY_BIT4 | \
+			 _PAGE_PKEY_BIT5 | \
+			 _PAGE_PKEY_BIT6 | \
+			 _PAGE_PKEY_BIT7 | \
+			 _PAGE_PKEY_BIT8 | \
+             _PAGE_PKEY_BIT9 )
+
+/* PHMon: end */
 
 /* Set of bits to preserve across pte_modify() */
 #define _PAGE_CHG_MASK  (~(unsigned long)(_PAGE_PRESENT | _PAGE_READ |	\
diff --git a/arch/riscv/include/asm/pgtable.h b/arch/riscv/include/asm/pgtable.h
index 1630196..5947ed0 100644
--- a/arch/riscv/include/asm/pgtable.h
+++ b/arch/riscv/include/asm/pgtable.h
@@ -32,6 +32,12 @@
 #include <asm/pgtable-32.h>
 #endif /* CONFIG_64BIT */
 
+/* PHMon: begin */
+#ifdef CONFIG_RISCV_MEMORY_PROTECTION_KEYS
+#include <linux/varanus.h>
+#endif
+/* PHMon: end */
+
 /* Number of entries in the page global directory */
 #define PTRS_PER_PGD    (PAGE_SIZE / sizeof(pgd_t))
 /* Number of entries in the page table */
@@ -89,6 +95,142 @@ extern pgd_t swapper_pg_dir[];
 #define __S110	PAGE_SHARED_EXEC
 #define __S111	PAGE_SHARED_EXEC
 
+/* PHMon: begin */
+//#define PKRU_AD_BIT 0x1
+//#define PKRU_WD_BIT 0x2
+#define PKRU_AD_BIT 0x2
+#define PKRU_WD_BIT 0x1
+#define PKRU_BITS_PER_PKEY 2
+#define PKRU_INDEX_BITS 0x5
+#define PKRU_ROW_BITS 0x5
+#define PKRU_ELEMENTS 0x20
+#define PKRU_ROWS 0x20
+
+//static inline bool __pkru_allows_read(u8 pkru_perm, u16 pkey)
+static inline bool __pkru_allows_read(u64 pkru, u16 pkey)
+{
+  bool result;
+  u64 pkru_pkey_bits = ((pkey << PKRU_INDEX_BITS) >> PKRU_INDEX_BITS) * PKRU_BITS_PER_PKEY;
+  
+  //result = pkru_perm & PKRU_AD_BIT;
+  //result = (u64)(pkru & (PKRU_AD_BIT << pkru_pkey_bits));
+  if ((u64)(pkru & ((PKRU_AD_BIT|PKRU_WD_BIT) << pkru_pkey_bits)) == 0) {
+    result = false;
+  }
+  else {
+    result = true;
+  }
+  #ifdef PHMON_DEBUG
+  printk("pkru_allows_read ---- ? 0x%d\n", result);
+  printk("pkru_allows_read ---- ? 0x%d pkru_pkey_bits: 0x%llx (AD_BITS << pkru_pkey_bits): 0x%x)\n", result, pkru_pkey_bits, PKRU_AD_BIT << pkru_pkey_bits);
+  #endif
+  return !result;
+  //return !(pkru & (PKRU_AD_BIT << pkru_pkey_bits));
+}
+
+//static inline bool __pkru_allows_write(u8 pkru_perm, u16 pkey)
+static inline bool __pkru_allows_write(u64 pkru, u16 pkey)
+{
+  bool result;
+  u64 pkru_pkey_bits = ((pkey << PKRU_INDEX_BITS) >> PKRU_INDEX_BITS) * PKRU_BITS_PER_PKEY;
+  /*
+   * Access-disable disables writes too so we need to check
+   * both bits here.
+   */
+  //result = pkru_perm & (PKRU_AD_BIT|PKRU_WD_BIT);
+  if ((u64)(pkru & ((PKRU_WD_BIT) << pkru_pkey_bits)) == 0) {
+    result = false;
+  }
+  else {
+    result = true;
+  }
+  //result = pkru & ((PKRU_AD_BIT|PKRU_WD_BIT) << pkru_pkey_bits);
+  /*if ((u64)(pkru & ((PKRU_AD_BIT|PKRU_WD_BIT) << pkru_pkey_bits)) == 0) {
+    result = false;
+  }
+  else {
+    result = true;
+    }*/
+  #ifdef PHMON_DEBUG
+  printk("pkru_allows_write ---- ? 0x%d pkru_pkey_bits: 0x%llx ((AD_BITS|WD_BITS) << pkru_pkey_bits): 0x%x)\n", result, pkru_pkey_bits, (PKRU_AD_BIT|PKRU_WD_BIT) << pkru_pkey_bits);
+  printk("pkru_allows_write ---- ? 0x%d\n", result);
+  #endif
+  return !result;
+  //return !(pkru & ((PKRU_AD_BIT|PKRU_WD_BIT) << pkru_pkey_bits));
+}
+
+static inline u16 pte_flags_pkey(unsigned long pte_flags)
+{
+#ifdef CONFIG_RISCV_MEMORY_PROTECTION_KEYS
+	/* ifdef to avoid doing 54-bit shift on 32-bit values */
+	return (pte_flags & _PAGE_PKEY_MASK) >> _PAGE_BIT_PKEY_BIT0;
+#else
+	return 0;
+#endif
+}
+
+static inline bool __pkru_allows_pkey(u16 pkey, bool write)
+{
+  //u8 pkru_perm = komodo_pkru_rd(pkey);
+  //u8 pkru_perm = 0x0;
+  u64 pkru_perm = 0x3;
+  #ifdef PHMON_DEBUG
+  printk("--- pkru_allows_pkey --- pkey: 0x%x write: %d\n", pkey, write);
+  #endif
+  if (pkey != 0)
+  {
+    pkru_perm = komodo_pkru_rd(pkey);
+    #ifdef PHMON_DEBUG
+    printk("--- pkru_allows_pkey --- after komodo_pkru_rd --- pkey_perm: 0x%llx\n", pkru_perm);
+    #endif
+
+    if (!__pkru_allows_read(pkru_perm, pkey))
+      return false;
+    if (write && !__pkru_allows_write(pkru_perm, pkey))
+      return false;
+  }
+
+  return true;
+}
+
+/*
+ * 'pteval' can come from a PTE, PMD or PUD.  We only check
+ * _PAGE_PRESENT, _PAGE_USER, and _PAGE_RW in here which are the
+ * same value on all 3 types.
+ */
+static inline bool __pte_access_permitted(unsigned long pteval, bool write)
+{
+  unsigned long need_pte_bits = _PAGE_PRESENT|_PAGE_USER;
+
+  if (write)
+    need_pte_bits |= _PAGE_WRITE;
+
+  if ((pteval & need_pte_bits) != need_pte_bits)
+    return 0;
+
+  return __pkru_allows_pkey(pte_flags_pkey(pteval), write);
+}
+
+#define pte_access_permitted pte_access_permitted
+static inline bool pte_access_permitted(pte_t pte, bool write)
+{
+	return __pte_access_permitted(pte_val(pte), write);
+}
+
+#define pmd_access_permitted pmd_access_permitted
+static inline bool pmd_access_permitted(pmd_t pmd, bool write)
+{
+	return __pte_access_permitted(pmd_val(pmd), write);
+}
+
+#define pud_access_permitted pud_access_permitted
+static inline bool pud_access_permitted(pud_t pud, bool write)
+{
+	return __pte_access_permitted(pud_val(pud), write);
+}
+
+/* PHMon: end */
+
 /*
  * ZERO_PAGE is a global shared page that is always zero,
  * used for zero-mapped memory areas, etc.
@@ -124,7 +266,7 @@ static inline void pmd_clear(pmd_t *pmdp)
 
 static inline pgd_t pfn_pgd(unsigned long pfn, pgprot_t prot)
 {
-	return __pgd((pfn << _PAGE_PFN_SHIFT) | pgprot_val(prot));
+    return __pgd((pfn << _PAGE_PFN_SHIFT) | pgprot_val(prot));
 }
 
 #define pgd_index(addr) (((addr) >> PGDIR_SHIFT) & (PTRS_PER_PGD - 1))
@@ -139,20 +281,43 @@ static inline pgd_t *pgd_offset(const struct mm_struct *mm, unsigned long addr)
 
 static inline struct page *pmd_page(pmd_t pmd)
 {
-	return pfn_to_page(pmd_val(pmd) >> _PAGE_PFN_SHIFT);
+    /* PHMon: begin */
+    //return pfn_to_page(pmd_val(pmd) >> _PAGE_PFN_SHIFT);
+
+    //printk("pfn: 0x%lx pfn_r: 0x%lx\n", pmd_val(pmd) >> _PAGE_PFN_SHIFT, (pmd_val(pmd) << _PAGE_PD_SIZE) >> _PAGE_PFN_R_SHIFT);
+    return pfn_to_page((pmd_val(pmd) << _PAGE_PD_SIZE) >> _PAGE_PFN_R_SHIFT);
+    /* PHMon: end */
 }
 
 static inline unsigned long pmd_page_vaddr(pmd_t pmd)
 {
-	return (unsigned long)pfn_to_virt(pmd_val(pmd) >> _PAGE_PFN_SHIFT);
+    /* PHMon: begin */
+	//return (unsigned long)pfn_to_virt(pmd_val(pmd) >> _PAGE_PFN_SHIFT);
+
+    //printk("pfn: 0x%lx pfn_r: 0x%lx \n", pmd_val(pmd) >> _PAGE_PFN_SHIFT, (pmd_val(pmd) << _PAGE_PD_SIZE) >> _PAGE_PFN_R_SHIFT);
+    return (unsigned long)pfn_to_virt((pmd_val(pmd) << _PAGE_PD_SIZE) >> _PAGE_PFN_R_SHIFT);
+    /* PHMon: end */
 }
 
 /* Yields the page frame number (PFN) of a page table entry */
 static inline unsigned long pte_pfn(pte_t pte)
 {
-	return (pte_val(pte) >> _PAGE_PFN_SHIFT);
+    /* PHMon: begin */
+	//return (pte_val(pte) >> _PAGE_PFN_SHIFT);
+
+    //printk("pte: 0x%lx pfn: 0x%lx new pfn: 0x%lx\n", pte_val(pte), pte_val(pte) >> _PAGE_PFN_SHIFT, (pte_val(pte) << _PAGE_PD_SIZE) >> _PAGE_PFN_R_SHIFT);
+    return ((pte_val(pte) << _PAGE_PD_SIZE )>> _PAGE_PFN_R_SHIFT);
+    /* PHMon: end */
 }
 
+/* PHMon: begin */
+/* Yields the protect domain (pd) of a page table entry */
+static inline unsigned long pte_pd(pte_t pte)
+{
+    return (pte_val(pte) >> _PAGE_PD_SHIFT);
+}
+/* PHMon: end */
+
 #define pte_page(x)     pfn_to_page(pte_pfn(x))
 
 /* Constructs a page table entry */
@@ -261,8 +426,24 @@ static inline pte_t pte_mkspecial(pte_t pte)
 /* Modify page protection bits */
 static inline pte_t pte_modify(pte_t pte, pgprot_t newprot)
 {
-	return __pte((pte_val(pte) & _PAGE_CHG_MASK) | pgprot_val(newprot));
+	return __pte((pte_val(pte) & _PAGE_CHG_MASK & ~(_PAGE_PKEY_MASK)) | pgprot_val(newprot));
+}
+
+/* PHMon: begin */
+/* mprotect needs to preserve PAT bits when updating vm_page_prot */
+#define pgprot_modify pgprot_modify
+static inline pgprot_t pgprot_modify(pgprot_t oldprot, pgprot_t newprot)
+{
+    // Different from x86 to ensure old pkey is overwritten
+    //pgprotval_t preservebits = pgprot_val(oldprot) & _PAGE_CHG_MASK;
+    pgprotval_t preservebits = (pgprot_val(oldprot) & _PAGE_CHG_MASK) & ~(_PAGE_PKEY_MASK);
+	pgprotval_t addbits = pgprot_val(newprot);
+    #ifdef PHMON_DEBUG
+    printk("---- pgprot_modify (pgtable.h) oldprot: 0x%lx mask: 0x%lx newprot: 0x%lx ---- return: 0x%lx\n", pgprot_val(oldprot), _PAGE_CHG_MASK, pgprot_val(newprot), pgprot_val(__pgprot(preservebits | addbits)));
+    #endif
+	return __pgprot(preservebits | addbits);
 }
+/* PHMon: end */
 
 #define pgd_ERROR(e) \
 	pr_err("%s:%d: bad pgd " PTE_FMT ".\n", __FILE__, __LINE__, pgd_val(e))
diff --git a/arch/riscv/include/asm/pkeys.h b/arch/riscv/include/asm/pkeys.h
new file mode 100644
index 0000000..a1cc0ec
--- /dev/null
+++ b/arch/riscv/include/asm/pkeys.h
@@ -0,0 +1,224 @@
+/* PHMon: begin */
+
+#ifndef _ASM_RISCV_PKEYS_H
+#define _ASM_RISCV_PKEYS_H
+
+#define arch_max_pkey() (1024)
+#define arch_pkey_rows() (32)
+#define arch_pkey_bits() (32)
+
+extern int arch_set_user_pkey_access(struct task_struct *tsk, int pkey,
+		unsigned long init_val);
+
+/*
+ * Try to dedicate one of the protection keys to be used as an
+ * execute-only protection key.
+ */
+extern int __execute_only_pkey(struct mm_struct *mm);
+static inline int execute_only_pkey(struct mm_struct *mm)
+{
+	return __execute_only_pkey(mm);
+}
+
+extern int __arch_override_mprotect_pkey(struct vm_area_struct *vma,
+		int prot, int pkey);
+static inline int arch_override_mprotect_pkey(struct vm_area_struct *vma,
+		int prot, int pkey)
+{
+	return __arch_override_mprotect_pkey(vma, prot, pkey);
+}
+
+extern int __arch_set_user_pkey_access(struct task_struct *tsk, int pkey,
+		unsigned long init_val);
+
+#define ARCH_VM_PKEY_FLAGS (VM_PKEY_BIT0 | VM_PKEY_BIT1 | VM_PKEY_BIT2 | VM_PKEY_BIT3 | VM_PKEY_BIT4 | VM_PKEY_BIT5 | VM_PKEY_BIT6 | VM_PKEY_BIT7 | VM_PKEY_BIT8 | VM_PKEY_BIT9)
+
+#define mm_pkey_allocation_map(mm, index)	(mm->context.pkey_allocation_map.row[index])
+#define mm_pkey_dirty_map_d(mm, index)	    (mm->context.pkey_dirty_map.row[index])
+#define mm_pkey_dirty_map_count(mm, index, select)	(mm->context.pkey_dirty_map.count[index][select])
+/*#define mm_set_pkey_allocated(mm, pkey) do {                          \
+    mm_pkey_allocation_map(mm, pkey/arch_pkey_rows()) |= (1U << (pkey%arch_pkey_rows())); \
+    } while (0)*/
+static inline
+void mm_set_pkey_allocated(struct mm_struct *mm, int pkey)
+{
+  int result;
+  result = (mm_pkey_allocation_map(mm, pkey/arch_pkey_rows()) | (1U << (pkey%arch_pkey_rows())));
+  #ifdef PHMON_DEBUG
+  printk("*** inside mm_set_pkey_allocated *** pkey_row: 0x%x\n", result);
+  #endif
+  mm->context.pkey_allocation_map.row[pkey/arch_pkey_rows()] = result;
+  #ifdef PHMON_DEBUG
+  printk("*** inside mm_set_pkey_allocated *** pkey_row after setting: 0x%x\n", mm_pkey_allocation_map(mm, pkey/arch_pkey_rows()));
+  #endif
+}
+
+/* 
+ * Enforce the lazy de-allocation approach.
+ * If there are available pages assoicated with the pkey, mark it as dirty.
+ * Do not free it yet.
+ */
+#define mm_set_pkey_free(mm, pkey) do {			                                              \
+    if (mm_pkey_dirty_map_count(mm, pkey/arch_pkey_rows(), pkey%arch_pkey_rows()) == 0) {     \
+      mm_pkey_allocation_map(mm, pkey/arch_pkey_rows()) &= ~(1U << (pkey%arch_pkey_rows()));  \
+      printk("**** inside mm_set_pkey_free: ok to de-alloc ****\n");                          \
+    } else {                                                                                  \
+      mm->context.pkey_dirty_map.row[pkey/arch_pkey_rows()] = mm_pkey_dirty_map_d(mm, pkey/arch_pkey_rows()) | (1U << (pkey%arch_pkey_rows()));                                                                \
+      printk("**** inside mm_set_pkey_free: lazy de-alloc: dirty? 0x%x counte: 0x%llx****\n", mm_pkey_dirty_map_d(mm, pkey/arch_pkey_rows()), mm_pkey_dirty_map_count(mm, pkey/arch_pkey_rows(), pkey%arch_pkey_rows())); \
+    }                                                                                         \
+} while (0)
+
+static inline
+bool mm_pkey_is_allocated(struct mm_struct *mm, int pkey)
+{
+	/*
+	 * "Allocated" pkeys are those that have been returned
+	 * from pkey_alloc().  pkey 0 is special, and never
+	 * returned from pkey_alloc().
+	 */
+	if (pkey <= 0)
+		return false;
+	if (pkey >= arch_max_pkey())
+		return false;
+    /* PHMon: begin */
+    #ifdef PHMON_DEBUG
+    printk("*** inside is_allocated **** pkey: 0x%x current: 0x%x element: 0x%x\n", pkey, mm_pkey_allocation_map(mm, pkey/arch_pkey_rows()), 1U << (pkey%arch_pkey_rows()));
+    #endif
+    /* PHMon: end */
+	return (mm_pkey_allocation_map(mm, pkey/arch_pkey_rows()) & (1U << (pkey%arch_pkey_rows())));
+}
+
+static inline
+bool mm_pkey_allocated(struct mm_struct *mm)
+{
+  int ii , cnt = 0;
+  if ((mm_pkey_allocation_map(mm, 0) == 0x1) || (mm_pkey_allocation_map(mm, 0) == 0x0)) {
+    cnt++;
+  }
+  for (ii = 1; ii < arch_pkey_rows(); ii++) {
+    if ((mm_pkey_allocation_map(mm, ii) == 0)) {
+      cnt++;
+    }
+  }
+  return !(cnt == arch_pkey_rows());   
+}
+
+/*
+ * Returns a positive, 10-bit key on success, or -1 on failure.
+ */
+static inline
+int mm_pkey_alloc(struct mm_struct *mm)
+{
+	/*
+	 * Note: this is the one and only place we make sure
+	 * that the pkey is valid as far as the hardware is
+	 * concerned.  The rest of the kernel trusts that
+	 * only good, valid pkeys come out of here.
+	 */
+     u32 all_pkeys_mask = ((1U << arch_pkey_bits()) - 1);
+     #ifdef PHMON_DEBUG
+     printk("all_pkeys_mask: 0x%x\n", all_pkeys_mask);
+     #endif
+
+	/*
+	 * Are we out of pkeys?  We must handle this specially
+	 * because ffz() behavior is undefined if there are no
+	 * zeros.
+	 */
+    int ii = 0;
+    int jj = 0;
+    int cnt_out = 0;
+
+    int rows = 0;
+    int ret = -1;
+    int ret_shift = 0;
+    int pos = 0;
+    
+    for (ii = 0; ii < arch_pkey_rows(); ii++) {
+      if (mm_pkey_allocation_map(mm, ii) == all_pkeys_mask) {
+        cnt_out++;
+      }
+    }
+    if (cnt_out == arch_pkey_rows()) {
+        #ifdef PHMON_DEBUG
+        printk("filled rows? %d\n", cnt_out);
+        #endif
+        /*
+         * All the pkeys are set in the allocation map, check the counters 
+         * in the dirty map. This is the lazy de-allocation approach.
+         */
+        for (ii = 0; ii < arch_pkey_rows(); ii++) {
+          for (jj = 0; jj < arch_pkey_bits(); jj++) {
+            printk("dirty map: 0x%x count: 0x%llx\n", mm_pkey_dirty_map_d(mm, ii), (mm_pkey_dirty_map_count(mm, ii, jj)));
+            if (mm_pkey_dirty_map_d(mm, ii) && (mm_pkey_dirty_map_count(mm, ii, jj) == 0)) {
+              mm->context.pkey_allocation_map.row[ii] = mm_pkey_allocation_map(mm, ii) & (~(1UL << (jj)));
+              printk("Found a zero counter in lazy de-allocation @ %d\n", ii*32 + jj);
+              break;
+            }
+          }
+        }
+        return -1;
+    }
+
+    while ((ret == -1) && rows < arch_pkey_rows()) {
+      ret_shift = ~(mm_pkey_allocation_map(mm, rows));
+      #ifdef PHMON_DEBUG
+      printk("rows: %d ret_shift: 0x%x\n", rows, ret_shift);
+      #endif
+      if ((ret_shift == 0xffff) && (rows != 0)) {
+        ret = rows * arch_pkey_rows();
+      }
+      else if (ret_shift != 0) {
+        while (!(ret_shift & 1)) {
+          ret_shift >>= 1;
+          ++pos;
+          #ifdef PHMON_DEBUG
+          printk("rows: %d ret_shift: 0x%x pos: %d\n", rows, ret_shift, pos);
+          #endif
+        }
+        ret = rows * arch_pkey_rows() + pos;
+      }
+      rows++;
+    }
+	//ret = ffz(mm_pkey_allocation_map(mm));
+    #ifdef PHMON_DEBUG
+    printk("****** pkey: 0x%x ******", ret);
+    #endif
+
+    int result = 0;
+    result = (mm_pkey_allocation_map(mm, (ret/arch_pkey_rows())) | (1U << (ret%arch_pkey_rows())));
+    #ifdef PHMON_DEBUG
+    printk("pkey: 0x%x pkey/rows: 0x%x rem(pkey,rows): 0x%x current: 0x%x shift: 0x%x result: 0x%x\n", ret, ret/arch_pkey_rows(), ret%arch_pkey_rows(), mm_pkey_allocation_map(mm, (ret/arch_pkey_rows())), (1U << (ret%arch_pkey_rows())), result);
+    #endif
+	mm_set_pkey_allocated(mm, ret);
+
+
+    result = (mm_pkey_allocation_map(mm, (ret/arch_pkey_rows())));
+    #ifdef PHMON_DEBUG
+    printk("pkey after setting: 0x%x\n", result);
+    printk("is allocated? 0x%x\n", mm_pkey_is_allocated(mm, ret));
+    #endif
+    
+	return ret;
+}
+
+static inline
+int mm_pkey_free(struct mm_struct *mm, int pkey)
+{
+	if (!mm_pkey_is_allocated(mm, pkey))
+		return -EINVAL;
+
+	mm_set_pkey_free(mm, pkey);
+
+	return 0;
+}
+
+extern int arch_set_user_pkey_access(struct task_struct *tsk, int pkey,
+		unsigned long init_val);
+extern int __arch_set_user_pkey_access(struct task_struct *tsk, int pkey,
+		unsigned long init_val);
+//extern void copy_init_pkru_to_fpregs(void);
+
+#endif /*_ASM_RISCV_PKEYS_H */
+
+/* PHMon: end */
diff --git a/arch/riscv/include/uapi/asm/mman.h b/arch/riscv/include/uapi/asm/mman.h
new file mode 100644
index 0000000..828a5a9
--- /dev/null
+++ b/arch/riscv/include/uapi/asm/mman.h
@@ -0,0 +1,44 @@
+#ifndef _ASM_RISCV_MMAN_H
+#define _ASM_RISCV_MMU_H
+
+//#define MAP_32BIT	0x40		/* only give out 32bit addresses */
+
+#ifdef CONFIG_RISCV_MEMORY_PROTECTION_KEYS
+/*
+ * Take the 10 protection key bits out of the vma->vm_flags
+ * value and turn them in to the bits that we can put in
+ * to a pte.
+ *
+ * Only override these if Protection Keys are available
+ * (which is only on 64-bit).
+ */
+#define arch_vm_get_page_prot(vm_flags)	__pgprot(	\
+		((vm_flags) & VM_PKEY_BIT0 ? _PAGE_PKEY_BIT0 : 0) |	\
+		((vm_flags) & VM_PKEY_BIT1 ? _PAGE_PKEY_BIT1 : 0) |	\
+		((vm_flags) & VM_PKEY_BIT2 ? _PAGE_PKEY_BIT2 : 0) |	\
+		((vm_flags) & VM_PKEY_BIT3 ? _PAGE_PKEY_BIT3 : 0) | \
+		((vm_flags) & VM_PKEY_BIT4 ? _PAGE_PKEY_BIT4 : 0) | \
+		((vm_flags) & VM_PKEY_BIT5 ? _PAGE_PKEY_BIT5 : 0) | \
+		((vm_flags) & VM_PKEY_BIT6 ? _PAGE_PKEY_BIT6 : 0) | \
+		((vm_flags) & VM_PKEY_BIT7 ? _PAGE_PKEY_BIT7 : 0) | \
+		((vm_flags) & VM_PKEY_BIT8 ? _PAGE_PKEY_BIT8 : 0) | \
+        ((vm_flags) & VM_PKEY_BIT9 ? _PAGE_PKEY_BIT9 : 0))
+
+#define arch_calc_vm_prot_bits(prot, key) (  	  \
+		((key) & 0x1 ? VM_PKEY_BIT0 : 0)   |      \
+		((key) & 0x2 ? VM_PKEY_BIT1 : 0)   |      \
+		((key) & 0x4 ? VM_PKEY_BIT2 : 0)   |      \
+		((key) & 0x8 ? VM_PKEY_BIT3 : 0)   |      \
+        ((key) & 0x16 ? VM_PKEY_BIT4 : 0)  |      \
+        ((key) & 0x32 ? VM_PKEY_BIT5 : 0)  |      \
+        ((key) & 0x64 ? VM_PKEY_BIT6 : 0)  |      \
+        ((key) & 0x128 ? VM_PKEY_BIT7 : 0) |      \
+        ((key) & 0x256 ? VM_PKEY_BIT8 : 0) |      \
+        ((key) & 0x512 ? VM_PKEY_BIT9 : 0))
+  
+#endif
+
+#include <asm-generic/mman.h>
+//#include <linux/mman.h>
+
+#endif /* _ASM_RISCV_MMAN_H */
diff --git a/arch/riscv/kernel/process.c b/arch/riscv/kernel/process.c
index d74d4ad..f946a73 100644
--- a/arch/riscv/kernel/process.c
+++ b/arch/riscv/kernel/process.c
@@ -76,7 +76,10 @@ void show_regs(struct pt_regs *regs)
 void start_thread(struct pt_regs *regs, unsigned long pc,
 	unsigned long sp)
 {
-	regs->sstatus = SR_SPIE /* User mode, irqs on */ | SR_FS_INITIAL;
+    /* PHMon: begin */
+	//regs->sstatus = SR_SPIE /* User mode, irqs on */ | SR_FS_INITIAL;
+    regs->sstatus = SR_SPIE /* User mode, irqs on */ | SR_FS_INITIAL | SR_XS_INITIAL;;
+    /* PHMon: end */
 	regs->sepc = pc;
 	regs->sp = sp;
 	set_fs(USER_DS);
diff --git a/arch/riscv/kernel/ptrace.c b/arch/riscv/kernel/ptrace.c
index ba3e807..9f82a7e 100644
--- a/arch/riscv/kernel/ptrace.c
+++ b/arch/riscv/kernel/ptrace.c
@@ -50,7 +50,7 @@ static int riscv_gpr_set(struct task_struct *target,
 	struct pt_regs *regs;
 
 	regs = task_pt_regs(target);
-	ret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, &regs, 0, -1);
+	ret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, regs, 0, -1);
 	return ret;
 }
 
diff --git a/arch/riscv/kernel/traps.c b/arch/riscv/kernel/traps.c
index 93132cb..75b79fa 100644
--- a/arch/riscv/kernel/traps.c
+++ b/arch/riscv/kernel/traps.c
@@ -149,8 +149,15 @@ asmlinkage void do_trap_break(struct pt_regs *regs)
 	}
 #endif /* CONFIG_GENERIC_BUG */
 
+    /* PHMon: begin */
+    /*if (current->counter > 2) {
+      struct timespec t1;
+      getnstimeofday(&t1);
+      printk("t1: %lld.%.9ld\n", (long long)t1.tv_sec, t1.tv_nsec);
+      }
+      current->counter++;*/
+    /* PHMon: end */
 	do_trap_siginfo(SIGTRAP, TRAP_BRKPT, regs->sepc, current);
-	regs->sepc += 0x4;
 }
 
 #ifdef CONFIG_GENERIC_BUG
diff --git a/arch/riscv/mm/Makefile b/arch/riscv/mm/Makefile
index 5e18092..b39e116 100644
--- a/arch/riscv/mm/Makefile
+++ b/arch/riscv/mm/Makefile
@@ -4,3 +4,4 @@ obj-y += extable.o
 obj-y += ioremap.o
 obj-y += cacheflush.o
 obj-y += dma.o
+obj-$(CONFIG_RISCV_MEMORY_PROTECTION_KEYS) += pkeys.o
diff --git a/arch/riscv/mm/fault.c b/arch/riscv/mm/fault.c
index 06c5621..2a57b1b 100644
--- a/arch/riscv/mm/fault.c
+++ b/arch/riscv/mm/fault.c
@@ -31,6 +31,26 @@
 #include <asm/ptrace.h>
 #include <asm/uaccess.h>
 
+/* PHMon: begin */
+#include <asm/mmu_context.h>   /* vma_pkey() */
+#include <linux/varanus.h>
+/* PHMon: end */
+
+/* PHMon: begin */
+static inline bool bad_area_access_from_pkeys(struct vm_area_struct *vma,
+       unsigned int flags)
+{
+	/* This code is always called on the current mm */
+
+	/* this checks permission keys on the VMA: */
+	if (!arch_vma_access_permitted(vma, flags & FAULT_FLAG_WRITE,
+					    flags & FAULT_FLAG_INSTRUCTION,
+					    flags & FAULT_FLAG_REMOTE))
+		return true;
+	return false;
+}
+/* PHmon: end */
+
 /*
  * This routine handles page faults.  It determines the address and the
  * problem, and then passes it off to one of the appropriate routines.
@@ -44,6 +64,12 @@ asmlinkage void do_page_fault(struct pt_regs *regs)
 	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
 	int fault, code = SEGV_MAPERR;
 
+    /* PHMon: begin */
+    #ifdef PHMON_DEBUG
+    printk("*************************** PAGE FAULT **************************\n");
+    #endif
+    /* PHMon: end */
+
 	cause = regs->scause;
 	addr = regs->sbadaddr;
 
@@ -95,6 +121,17 @@ asmlinkage void do_page_fault(struct pt_regs *regs)
 	 * we can handle it.
 	 */
 good_area:
+    /* PHMon: begin */
+    #ifdef PHMON_DEBUG
+    printk("***** do_page_fault (fault.c): good_area ****\n");
+    #endif
+    if (bad_area_access_from_pkeys(vma, flags))
+    {
+        code = SEGV_PKUERR;
+        goto bad_area;
+    }
+    /* PHMon: end */
+    
 	code = SEGV_ACCERR;
 
 	switch (cause) {
@@ -103,10 +140,25 @@ asmlinkage void do_page_fault(struct pt_regs *regs)
 			goto bad_area;
 		break;
 	case EXC_LOAD_PAGE_FAULT:
+      /* PHMon: begin */
+      #ifdef PHMON_DEBUG
+      printk("**** load page fault ****\n");
+      #endif
+      /* PHMon: end */
 		if (!(vma->vm_flags & VM_READ))
 			goto bad_area;
+        /* PHMon: begin */
+        #ifdef PHMON_DEBUG
+        printk("/////////////////////////////////////// unknown load page fault ////////////////////////// \n");
+        #endif
+      /* PHMon: end */
 		break;
 	case EXC_STORE_PAGE_FAULT:
+      /* PHMon: begin */
+      #ifdef PHMON_DEBUG
+      printk("**** store page fault ****\n");
+      #endif
+      /* PHMon: end */
 		if (!(vma->vm_flags & VM_WRITE))
 			goto bad_area;
 		flags |= FAULT_FLAG_WRITE;
@@ -122,6 +174,11 @@ asmlinkage void do_page_fault(struct pt_regs *regs)
 	 */
 	fault = handle_mm_fault(vma, addr, flags);
 
+    /* PHMon: begin */
+    #ifdef PHMON_DEBUG
+    printk("do_page_fault (fault.c): fault? %d\n", fault);
+    #endif
+    /* PHMon: end */
 	/*
 	 * If we need to retry but a fatal signal is pending, handle the
 	 * signal first. We do not need to release the mmap_sem because it
@@ -178,6 +235,11 @@ asmlinkage void do_page_fault(struct pt_regs *regs)
 	 * Fix it, but check if it's kernel or user first.
 	 */
 bad_area:
+    /* PHMon: begin */
+    #ifdef PHMON_DEBUG
+    printk("**** do_page_fault (fault.c): bad_area ****\n");
+    #endif
+    /* PHMon: end */
 	up_read(&mm->mmap_sem);
 	/* User mode accesses just cause a SIGSEGV */
 	if (user_mode(regs)) {
diff --git a/arch/riscv/mm/pkeys.c b/arch/riscv/mm/pkeys.c
new file mode 100644
index 0000000..19093e0
--- /dev/null
+++ b/arch/riscv/mm/pkeys.c
@@ -0,0 +1,270 @@
+/*
+ * RISC-V Memory Protection Keys management
+ */
+#include <linux/debugfs.h>		/* debugfs_create_u32()		*/
+#include <linux/mm_types.h>             /* mm_struct, vma, etc...       */
+#include <linux/pkeys.h>                /* PKEY_*                       */
+#include <uapi/asm-generic/mman-common.h>
+
+#include <asm/mmu_context.h>            /* vma_pkey()                   */
+#include <linux/uaccess.h>
+#include <linux/varanus.h>
+
+#ifdef CONFIG_ARCH_HAS_PKEYS
+
+/*
+ * This will go out and modify PKRU register to set the access
+ * rights for @pkey to @init_val.
+ */
+int arch_set_user_pkey_access(struct task_struct *tsk, int pkey,
+		unsigned long init_val)
+{
+    u64 old_pkru;
+    int pkey_shift = (((pkey << PKRU_INDEX_BITS) >> PKRU_INDEX_BITS) * PKRU_BITS_PER_PKEY);
+	u64 new_pkru_bits = init_val;
+
+	/* Set the bits we need in PKRU:  */
+	/*if (init_val & PKEY_DISABLE_ACCESS)
+		new_pkru_bits |= PKRU_AD_BIT;
+	if (init_val & PKEY_DISABLE_WRITE)
+    new_pkru_bits |= PKRU_WD_BIT;*/
+
+	/* Shift the bits in to the correct place in PKRU for pkey: */
+	new_pkru_bits <<= pkey_shift;
+
+	/* Get old PKRU and mask off any old bits in place: */
+	//old_pkru = read_pkru();
+    old_pkru = komodo_pkru_rd(pkey);
+    #ifdef PHMON_DEBUG
+    printk("init_value: 0x%lx pkey_shift: %d new_pkru_bits: 0x%llx old_pkru: 0x%llx\n", init_val, pkey_shift, new_pkru_bits, old_pkru);
+    #endif
+	old_pkru &= ~(((u64)(PKRU_AD_BIT|PKRU_WD_BIT) << pkey_shift));
+    #ifdef PHMON_DEBUG
+    printk("old_pkru afeter shift: 0x%llx write value: 0x%llx\n", old_pkru, old_pkru | new_pkru_bits);
+    #endif
+
+	/* Write old part along with new part: */
+	//write_pkru(old_pkru | new_pkru_bits);
+
+    komodo_pkru_wr(pkey, old_pkru | new_pkru_bits);
+
+	return 0;
+}
+#endif /* ! CONFIG_ARCH_HAS_PKEYS */
+
+int __execute_only_pkey(struct mm_struct *mm)
+{
+    bool need_to_set_mm_pkey = false;
+    int execute_only_pkey = mm->context.execute_only_pkey;
+    int ret;
+
+    #ifdef PHMON_DEBUG
+    printk("---- __execute_only_pkey (pkey.c) ---- execute_only_pkey: 0x%x\n", execute_only_pkey);
+    #endif
+	/* Do we need to assign a pkey for mm's execute-only maps? */
+	if (execute_only_pkey == -1) {
+		/* Go allocate one to use, which might fail */
+      execute_only_pkey = mm_pkey_alloc(mm);
+      #ifdef PHMON_DEBUG
+      printk("---- __execute_only_pkey (pkey.c) ---- allocated new execute_only_pkey: 0x%x\n", execute_only_pkey);
+      #endif
+      if (execute_only_pkey < 0)
+        return -1;
+      need_to_set_mm_pkey = true;
+    }
+
+	/*
+	 * We do not want to go through the relatively costly
+	 * dance to set PKRU if we do not need to.  Check it
+	 * first and assume that if the execute-only pkey is
+	 * write-disabled that we do not have to set it
+	 * ourselves.  We need preempt off so that nobody
+	 * can make fpregs inactive.
+	 */
+	preempt_disable();
+	if (!need_to_set_mm_pkey &&
+	    !__pkru_allows_read(komodo_pkru_rd(execute_only_pkey), execute_only_pkey)) {
+		preempt_enable();
+		return execute_only_pkey;
+	}
+	preempt_enable();
+
+	/*
+	 * Set up PKRU so that it denies access for everything
+	 * other than execution.
+	 */
+	ret = arch_set_user_pkey_access(current, execute_only_pkey,
+			PKEY_DISABLE_ACCESS|PKEY_DISABLE_WRITE);
+    #ifdef PHMON_DEBUG
+    printk("---- __execute_only_pkey (pkey.c) ---- execute_only_pkey pkey(0x%x): 0x%x\n", execute_only_pkey, ret);
+    #endif
+	/*
+	 * If the PKRU-set operation failed somehow, just return
+	 * 0 and effectively disable execute-only support.
+	 */
+	if (ret) {
+		mm_set_pkey_free(mm, execute_only_pkey);
+		return -1;
+    }
+
+	/* We got one, store it and use it from here on out */
+	if (need_to_set_mm_pkey)
+		mm->context.execute_only_pkey = execute_only_pkey;
+        return execute_only_pkey;
+    return 0;
+}
+
+static inline bool vma_is_pkey_exec_only(struct vm_area_struct *vma)
+{
+	/* Do this check first since the vm_flags should be hot */
+	if ((vma->vm_flags & (VM_READ | VM_WRITE | VM_EXEC)) != VM_EXEC)
+		return false;
+	if (vma_pkey(vma) != vma->vm_mm->context.execute_only_pkey)
+        return false;
+
+	return true;
+}
+
+/*
+ * This is only called for *plain* mprotect calls.
+ */
+int __arch_override_mprotect_pkey(struct vm_area_struct *vma, int prot, int pkey)
+{
+	/*
+	 * Is this an mprotect_pkey() call?  If so, never
+	 * override the value that came from the user.
+	 */
+	if (pkey != -1)
+		return pkey;
+	/*
+	 * Look for a protection-key-drive execute-only mapping
+	 * which is now being given permissions that are not
+	 * execute-only.  Move it back to the default pkey.
+	 */
+	if (vma_is_pkey_exec_only(vma) &&
+	    (prot & (PROT_READ|PROT_WRITE))) {
+        #ifdef PHMON_DEBUG
+      printk("---- arch_override_mprotect_pkey (pkey.c) ---- change an execute only page to something else, switch pkey to default\n");
+        #endif
+		return 0;
+    }
+	/*
+	 * The mapping is execute-only.  Go try to get the
+	 * execute-only protection key.  If we fail to do that,
+	 * fall through as if we do not have execute-only
+	 * support.
+	 */
+	if (prot == PROT_EXEC) {
+		pkey = execute_only_pkey(vma->vm_mm);
+        #ifdef PHMON_DEBUG
+        printk("---- arch_override_mprotect_pkey (pkey.c) ---- execute only, pkey: 0x%x\n", pkey);
+        #endif
+		if (pkey > 0)
+			return pkey;
+    }
+	/*
+	 * This is a vanilla, non-pkey mprotect (or we failed to
+	 * setup execute-only), inherit the pkey from the VMA we
+	 * are working on.
+	 */
+	return vma_pkey(vma);
+}
+
+#define PKRU_AD_KEY(pkey)	(((u64)PKRU_AD_BIT) << ((pkey) * PKRU_BITS_PER_PKEY))
+
+/*
+ * Make the default PKRU value (at execve() time) as restrictive
+ * as possible.  This ensures that any threads clone()'d early
+ * in the process's lifetime will not accidentally get access
+ * to data which is pkey-protected later on.
+ */
+u64 init_pkru_value[32] = { [0 ... 31] = PKRU_AD_KEY( 1) | PKRU_AD_KEY( 2) |
+              PKRU_AD_KEY( 3) | PKRU_AD_KEY( 4) | PKRU_AD_KEY( 5) |
+              PKRU_AD_KEY( 6) | PKRU_AD_KEY( 7) | PKRU_AD_KEY( 8) |
+              PKRU_AD_KEY( 9) | PKRU_AD_KEY(10) | PKRU_AD_KEY(11) |
+              PKRU_AD_KEY(12) | PKRU_AD_KEY(13) | PKRU_AD_KEY(14) |
+              PKRU_AD_KEY(15) | PKRU_AD_KEY(16) | PKRU_AD_KEY(17) |
+              PKRU_AD_KEY(18) | PKRU_AD_KEY(19) | PKRU_AD_KEY(20) |
+              PKRU_AD_KEY(21) | PKRU_AD_KEY(22) | PKRU_AD_KEY(23) |
+              PKRU_AD_KEY(24) | PKRU_AD_KEY(25) | PKRU_AD_KEY(26) |
+              PKRU_AD_KEY(27) | PKRU_AD_KEY(28) | PKRU_AD_KEY(29) |
+              PKRU_AD_KEY(30) | PKRU_AD_KEY(31) | PKRU_AD_KEY(0)};
+
+static ssize_t init_pkru_read_file(struct file *file, char __user *user_buf,
+			     size_t count, loff_t *ppos)
+{
+	char buf[2048] = "";
+	unsigned int len;
+    int rows = 32;
+
+    int ii = 0;
+    len = sprintf(buf + strlen(buf), "0x%llx\n", (init_pkru_value[0] ^ PKRU_AD_KEY(0)));
+    for (ii = 1; ii < rows; ii++) {
+      len += sprintf(buf + strlen(buf), "0x%llx\n", init_pkru_value[ii]);
+    }
+    
+	return simple_read_from_buffer(user_buf, count, ppos, buf, len);
+}
+
+static ssize_t init_pkru_write_file(struct file *file,
+		 const char __user *user_buf, size_t count, loff_t *ppos)
+{
+	char buf[2048];
+	ssize_t len;
+	u64 new_init_pkru[32];
+    int ii;
+
+	len = min(count, sizeof(buf) - 1);
+	if (__copy_from_user(buf, user_buf, len))
+		return -EFAULT;
+
+	/* Make the buffer a valid string that we can not overrun */
+	buf[len] = '\0';
+	if (kstrtoull(buf, 0, &(new_init_pkru[0])))
+		return -EINVAL;
+
+	/*
+	 * Don't allow insane settings that will blow the system
+	 * up immediately if someone attempts to disable access
+	 * or writes to pkey 0.
+	 */
+    for (ii = 0; ii < 32; ii++) {
+      if (new_init_pkru[ii] & (PKRU_AD_BIT|PKRU_WD_BIT))
+		return -EINVAL;
+    }
+
+    for (ii = 0; ii < 32; ii++) {
+      WRITE_ONCE(init_pkru_value[ii], new_init_pkru[ii]);
+    }
+	return count;
+}
+
+static const struct file_operations fops_init_pkru = {
+	.read = init_pkru_read_file,
+	.write = init_pkru_write_file,
+	.llseek = default_llseek,
+};
+
+/*static int __init create_init_pkru_value(void)
+{
+	debugfs_create_file("init_pkru", S_IRUSR | S_IWUSR,
+			arch_debugfs_dir, NULL, &fops_init_pkru);
+	return 0;
+}
+late_initcall(create_init_pkru_value);*/
+
+static __init int setup_init_pkru(char *opt)
+{
+	u64 new_init_pkru[32];
+    int ii;
+
+	if (kstrtoull(opt, 0, &(new_init_pkru[0])))
+		return 1;
+
+    for (ii = 0; ii < 32; ii++) {
+      WRITE_ONCE(init_pkru_value[ii], new_init_pkru[ii]);
+    }
+
+	return 1;
+}
+__setup("init_pkru=", setup_init_pkru);
diff --git a/drivers/irqchip/irq-riscv-intc.c b/drivers/irqchip/irq-riscv-intc.c
index fec897d..1b4ffb5 100644
--- a/drivers/irqchip/irq-riscv-intc.c
+++ b/drivers/irqchip/irq-riscv-intc.c
@@ -23,6 +23,9 @@
 #include <asm/ptrace.h>
 #include <asm/sbi.h>
 #include <asm/smp.h>
+/* PHMon: begin */
+#include <linux/varanus.h>
+/* PHMon: end */
 
 #define PTR_BITS (8 * sizeof(uintptr_t))
 
@@ -81,6 +84,32 @@ void riscv_intc_irq(struct pt_regs *regs)
 	case INTERRUPT_CAUSE_SOFTWARE:
 		riscv_software_interrupt();
 		break;
+    /* PHMon: begin */
+    case INTERRUPT_CAUSE_ROCC:
+      if (strcmp(current->comm, "ndisasm") != 0) {
+        printk("CATCH ROCC INTERRUPT\n");
+        kill_pid(find_vpid(current->pid), SIGQUIT, 1);
+      }
+      /*else if (strcmp(current->comm, "dump.rv") == 0) {
+        kill_pid(find_vpid(current->pid), SIGTRAP, 1);
+        kill_pid(find_vpid(current->pid), SIGKILL, 1);
+      }*/
+      else {
+        komodo_resume();
+        unsigned long varanus_done = 0;
+	    komodo_done_info(varanus_done);
+        int i = 0;
+        while (varanus_done != 0) {
+          i = 0;
+          while (i < 10000) {
+            i++;
+            asm volatile ("slli x0, x1, 0");
+          }
+          komodo_done_info(varanus_done);
+        }
+      }
+      break;
+    /* PHMon: end */
 	default:
 		domain = per_cpu(riscv_irq_data, smp_processor_id()).domain;
 		generic_handle_irq(irq_find_mapping(domain, cause));
@@ -89,6 +118,7 @@ void riscv_intc_irq(struct pt_regs *regs)
 
 	irq_exit();
 	set_irq_regs(old_regs);
+    /* PHMon: end */
 }
 
 static int riscv_irqdomain_map(struct irq_domain *d, unsigned int irq,
diff --git a/fs/Makefile b/fs/Makefile
index ef772f1..c44409c 100644
--- a/fs/Makefile
+++ b/fs/Makefile
@@ -12,7 +12,8 @@ obj-y :=	open.o read_write.o file_table.o super.o \
 		attr.o bad_inode.o file.o filesystems.o namespace.o \
 		seq_file.o xattr.o libfs.o fs-writeback.o \
 		pnode.o splice.o sync.o utimes.o \
-		stack.o fs_struct.o statfs.o fs_pin.o nsfs.o
+		stack.o fs_struct.o statfs.o fs_pin.o nsfs.o \
+		komodo.o
 
 ifeq ($(CONFIG_BLOCK),y)
 obj-y +=	buffer.o block_dev.o direct-io.o mpage.o
diff --git a/fs/exec.c b/fs/exec.c
index 7eb8d21..83417bf 100644
--- a/fs/exec.c
+++ b/fs/exec.c
@@ -71,6 +71,13 @@
 #include "internal.h"
 
 #include <trace/events/sched.h>
+/* PHMon: begin */
+#include <linux/varanus.h>
+#ifdef CONFIG_RISCV_MEMORY_PROTECTION_KEYS
+#include <asm/pgtable.h>
+#define is_user_mode() ((csr_read(sstatus) & SR_SPP) == 0)
+#endif
+/* PHMon: end */
 
 int suid_dumpable = 0;
 
@@ -1803,6 +1810,20 @@ static int do_execveat_common(int fd, struct filename *filename,
 	if (retval < 0)
 		goto out;
 
+    /* PHMon: begin */
+    #ifdef CONFIG_RISCV_MEMORY_PROTECTION_KEYS
+    int i;
+    if(is_user_mode() && (current->mm != NULL)) {
+      for (i = 0; i < PKRU_ROWS; i++) {
+        current->pkru_state[i].row = 0;
+      }
+      asm volatile ("nop");
+      //printk("start thread[%s] pid: 0x%x, tgid: 0x%x\n", current->comm, current->pid, current->tgid);
+    }
+    #endif
+    
+    /* PHMon: end*/
+    
 	/* execve succeeded */
 	current->fs->in_exec = 0;
 	current->in_execve = 0;
@@ -1925,7 +1952,22 @@ SYSCALL_DEFINE3(execve,
 		const char __user *const __user *, argv,
 		const char __user *const __user *, envp)
 {
-	return do_execve(getname(filename), argv, envp);
+
+  /* PHMon: begin */
+    #ifdef CONFIG_RISCV_MEMORY_PROTECTION_KEYS
+    int i;
+    if(is_user_mode() && (current->mm != NULL)) {
+      for (i = 0; i < PKRU_ROWS; i++) {
+        current->pkru_state[i].row = 0;
+      }
+      #ifdef PHMON_DEBUG
+      printk("start thread[%s] pid: 0x%x, tgid: 0x%x\n", current->comm, current->pid, current->tgid);
+      #endif
+    }
+    #endif
+  return res;
+  //return do_execve(getname(filename), argv, envp);
+  /* PHMon: end */
 }
 
 SYSCALL_DEFINE5(execveat,
diff --git a/fs/komodo.c b/fs/komodo.c
new file mode 100644
index 0000000..af03e5c
--- /dev/null
+++ b/fs/komodo.c
@@ -0,0 +1,112 @@
+#include <linux/varanus.h>
+
+#define komodo_set_config(comparator, action, data) \
+  ROCC_INSTRUCTION_0_R_R(XCUSTOM_KOMODO, ((uint64_t) action << 32 | comparator), data, 5, 11, 12);
+
+#define komodo_set_mask(comparator, action, data) \
+  ROCC_INSTRUCTION_0_R_R(XCUSTOM_KOMODO, ((uint64_t) action << 32 | comparator), data, 0, 11, 12);
+
+#define komodo_ctrl(comparator, action) \
+  ROCC_INSTRUCTION_0_R_R(XCUSTOM_KOMODO, ((uint64_t) action << 32 | comparator), 0, 1, 11, 12);
+
+#define komodo_ctrl_wr(comparator, action, data) \
+  ROCC_INSTRUCTION_0_R_R(XCUSTOM_KOMODO, ((uint64_t) action << 32 | comparator), data, 1, 11, 12);
+
+#define komodo_ctrl_rd(comparator, action, out) \
+  ROCC_INSTRUCTION(XCUSTOM_KOMODO, out, ((uint64_t) action << 32 | comparator), 0, 1);
+
+void komodo_action_config(uint32_t comparator, act_conf_table_t * action) {
+  komodo_set_config(comparator, e_SC_TYPE, action->op_type);
+  komodo_set_config(comparator, e_SC_IN1, action->in1);
+  komodo_set_config(comparator, e_SC_IN2, action->in2);
+  komodo_set_config(comparator, e_SC_FN, action->fn);
+  komodo_set_config(comparator, e_SC_OUT, action->out);
+  komodo_set_config(comparator, e_SC_DATA, action->data);
+  komodo_set_config(comparator, e_SC_DONE, 0);
+}
+
+void komodo_pattern(uint32_t comparator, mask_t * mask) {
+  komodo_set_mask(comparator, e_SM_PC_SRC_CARE, mask->care.pc_src);
+  komodo_set_mask(comparator, e_SM_PC_DST_CARE, mask->care.pc_dst);
+  komodo_set_mask(comparator, e_SM_INST_CARE, mask->care.inst);
+  komodo_set_mask(comparator, e_SM_RD_CARE, mask->care.rd);
+  komodo_set_mask(comparator, e_SM_DATA_CARE, mask->care.data);
+  komodo_set_mask(comparator, e_SM_PC_SRC_DCARE, mask->dont_care.pc_src);
+  komodo_set_mask(comparator, e_SM_PC_DST_DCARE, mask->dont_care.pc_dst);
+  komodo_set_mask(comparator, e_SM_INST_DCARE, mask->dont_care.inst);
+  komodo_set_mask(comparator, e_SM_RD_DCARE, mask->dont_care.rd);
+  komodo_set_mask(comparator, e_SM_DATA_DCARE, mask->dont_care.data);
+}
+
+void komodo_enable(uint32_t comparator) {
+  komodo_ctrl(comparator, e_C_VALID);
+}
+
+void komodo_disable(uint32_t comparator) {
+  komodo_ctrl(comparator, e_C_INVALID);
+}
+
+void komodo_reset_val(uint32_t comparator) {
+  komodo_ctrl(comparator, e_C_RESET);
+}
+
+void komodo_match_count(uint32_t comparator, data_t count, xlen_t * addr) {
+  komodo_ctrl_wr(comparator, e_C_M_COUNT, count);
+}
+
+void komodo_set_local_reg(uint32_t index, xlen_t *addr)
+{
+  komodo_ctrl_wr(index, e_C_LOCAL, addr);
+}
+
+void komodo_set_mem_typ(xlen_t type) {
+  komodo_ctrl_wr(0, e_C_MEM_TYPE, type);
+}
+
+void komodo_set_commit_index(uint32_t comparator, data_t index) {
+  komodo_ctrl_wr(comparator, e_C_COMMIT_IDX, index);
+}
+
+uint64_t komodo_info_sp_offset(uint32_t index) {
+  uint64_t out;
+  komodo_ctrl_rd(index, e_C_INFO_SP_OFFSET, out);
+  return out;
+}
+
+uint64_t komodo_wait_resp_info(uint64_t  action) {
+  uint64_t out;
+  ROCC_INSTRUCTION(XCUSTOM_KOMODO, out, action, 0, 9);
+  return out;
+}
+
+void komodo_mem_cmd(uint64_t  addr, uint64_t data) {
+  ROCC_INSTRUCTION_0_R_R(XCUSTOM_KOMODO, addr, data, 11, 11, 12);
+}
+
+uint64_t komodo_read_mask(uint32_t comparator, uint64_t action) {
+  uint64_t out;
+  ROCC_INSTRUCTION(XCUSTOM_KOMODO, out, ((uint64_t) action << 32 | comparator), 0, 12);
+  return out;
+}
+
+uint64_t komodo_read_conf(uint32_t comparator, uint64_t action) {
+  uint64_t out;
+  ROCC_INSTRUCTION(XCUSTOM_KOMODO, out, ((uint64_t) action << 32 | comparator), 0, 13);
+  return out;
+}
+
+uint64_t komodo_read_commit_index(uint32_t comparator) {
+  uint64_t out;
+  ROCC_INSTRUCTION(XCUSTOM_KOMODO, out, ((uint64_t) comparator), 0, 14);
+  return out;
+}
+
+uint64_t komodo_pkru_rd(uint64_t pkey) {
+  uint64_t out;
+  ROCC_INSTRUCTION_R_R_R(XCUSTOM_KOMODO, out, ((uint64_t) pkey), 0, 17, 13, 11, 12);
+  return out;
+}
+
+void komodo_pkru_wr(uint64_t pkey, uint64_t perm) {
+  ROCC_INSTRUCTION_0_R_R(XCUSTOM_KOMODO, ((uint64_t) pkey), ((uint64_t) perm), 16, 11, 12);
+}
diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c
index 339e4c1..9e75378 100644
--- a/fs/proc/task_mmu.c
+++ b/fs/proc/task_mmu.c
@@ -681,6 +681,19 @@ static void show_smap_vma_flags(struct seq_file *m, struct vm_area_struct *vma)
 		[ilog2(VM_PKEY_BIT2)]	= "",
 		[ilog2(VM_PKEY_BIT3)]	= "",
 #endif
+#ifdef CONFIG_RISCV_MEMORY_PROTECTION_KEYS
+		/* These come out via ProtectionKey: */
+		[ilog2(VM_PKEY_BIT0)]	= "",
+		[ilog2(VM_PKEY_BIT1)]	= "",
+		[ilog2(VM_PKEY_BIT2)]	= "",
+		[ilog2(VM_PKEY_BIT3)]	= "",
+        [ilog2(VM_PKEY_BIT4)]	= "",
+		[ilog2(VM_PKEY_BIT5)]	= "",
+		[ilog2(VM_PKEY_BIT6)]	= "",
+		[ilog2(VM_PKEY_BIT7)]	= "",
+        [ilog2(VM_PKEY_BIT8)]	= "",
+		[ilog2(VM_PKEY_BIT9)]	= "",
+#endif
 	};
 	size_t i;
 
diff --git a/include/linux/mm.h b/include/linux/mm.h
index ea818ff..c4f0b2d 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -227,6 +227,20 @@ extern unsigned int kobjsize(const void *objp);
 #define VM_HIGH_ARCH_3	BIT(VM_HIGH_ARCH_BIT_3)
 #define VM_HIGH_ARCH_4	BIT(VM_HIGH_ARCH_BIT_4)
 #endif /* CONFIG_ARCH_USES_HIGH_VMA_FLAGS */
+/* PHMon: begin */
+#ifdef CONFIG_RISCV_MEMORY_PROTECTION_KEYS
+#define VM_HIGH_ARCH_BIT_5	37	/* bit only usable on 64-bit architectures */
+#define VM_HIGH_ARCH_BIT_6	38	/* bit only usable on 64-bit architectures */
+#define VM_HIGH_ARCH_BIT_7	39	/* bit only usable on 64-bit architectures */
+#define VM_HIGH_ARCH_BIT_8	40	/* bit only usable on 64-bit architectures */
+#define VM_HIGH_ARCH_BIT_9	41	/* bit only usable on 64-bit architectures */
+#define VM_HIGH_ARCH_5	BIT(VM_HIGH_ARCH_BIT_5)
+#define VM_HIGH_ARCH_6	BIT(VM_HIGH_ARCH_BIT_6)
+#define VM_HIGH_ARCH_7	BIT(VM_HIGH_ARCH_BIT_7)
+#define VM_HIGH_ARCH_8	BIT(VM_HIGH_ARCH_BIT_8)
+#define VM_HIGH_ARCH_9	BIT(VM_HIGH_ARCH_BIT_9)
+#endif
+/* PHMon: end */
 
 #if defined(CONFIG_X86)
 # define VM_PAT		VM_ARCH_1	/* PAT reserves whole VMA at once (x86) */
@@ -249,6 +263,22 @@ extern unsigned int kobjsize(const void *objp);
 # define VM_MAPPED_COPY	VM_ARCH_1	/* T if mapped copy of data (nommu mmap) */
 #endif
 
+/* PHMon: begin */
+#if defined (CONFIG_RISCV_MEMORY_PROTECTION_KEYS)
+# define VM_PKEY_SHIFT	VM_HIGH_ARCH_BIT_0
+# define VM_PKEY_BIT0	VM_HIGH_ARCH_0	/* A protection key is a 10-bit value */
+# define VM_PKEY_BIT1	VM_HIGH_ARCH_1
+# define VM_PKEY_BIT2	VM_HIGH_ARCH_2
+# define VM_PKEY_BIT3	VM_HIGH_ARCH_3
+# define VM_PKEY_BIT4	VM_HIGH_ARCH_4
+# define VM_PKEY_BIT5	VM_HIGH_ARCH_5
+# define VM_PKEY_BIT6	VM_HIGH_ARCH_6
+# define VM_PKEY_BIT7	VM_HIGH_ARCH_7
+# define VM_PKEY_BIT8	VM_HIGH_ARCH_8
+# define VM_PKEY_BIT9	VM_HIGH_ARCH_9
+#endif
+/* PHMon: end */
+
 #if defined(CONFIG_X86_INTEL_MPX)
 /* MPX specific bounds table or bounds directory */
 # define VM_MPX		VM_HIGH_ARCH_4
diff --git a/include/linux/mman.h b/include/linux/mman.h
index 6a4d1ca..a8a45a8 100644
--- a/include/linux/mman.h
+++ b/include/linux/mman.h
@@ -8,6 +8,12 @@
 #include <linux/atomic.h>
 #include <uapi/linux/mman.h>
 
+/* PHMon: begin */
+#ifdef CONFIG_RISCV_MEMORY_PROTECTION_KEYS
+#include <linux/varanus.h>
+#endif
+/* PHMon: end */
+
 /*
  * Arrange for legacy / undefined architecture specific flags to be
  * ignored by mmap handling code.
@@ -77,13 +83,51 @@ static inline void vm_unacct_memory(long pages)
  * Allow architectures to handle additional protection bits
  */
 
+/* PHMon: begin */
+//#ifndef arch_calc_vm_prot_bits
+//#define arch_calc_vm_prot_bits(prot, pkey) 0
+//#endif
+
 #ifndef arch_calc_vm_prot_bits
+#ifdef CONFIG_RISCV_MEMORY_PROTECTION_KEYS
+#define arch_calc_vm_prot_bits(prot, key) (  	  \
+		((key) & 0x1 ? VM_PKEY_BIT0 : 0)   |      \
+		((key) & 0x2 ? VM_PKEY_BIT1 : 0)   |      \
+		((key) & 0x4 ? VM_PKEY_BIT2 : 0)   |      \
+		((key) & 0x8 ? VM_PKEY_BIT3 : 0)   |      \
+        ((key) & 0x10 ? VM_PKEY_BIT4 : 0)  |      \
+        ((key) & 0x20 ? VM_PKEY_BIT5 : 0)  |      \
+        ((key) & 0x40 ? VM_PKEY_BIT6 : 0)  |      \
+        ((key) & 0x80 ? VM_PKEY_BIT7 : 0)  |      \
+        ((key) & 0x100 ? VM_PKEY_BIT8 : 0) |      \
+        ((key) & 0x200 ? VM_PKEY_BIT9 : 0))
+#else
 #define arch_calc_vm_prot_bits(prot, pkey) 0
 #endif
+#endif
+
+//#ifndef arch_vm_get_page_prot
+//#define arch_vm_get_page_prot(vm_flags) __pgprot(0)
+//#endif
 
 #ifndef arch_vm_get_page_prot
+#ifdef CONFIG_RISCV_MEMORY_PROTECTION_KEYS
+#define arch_vm_get_page_prot(vm_flags)	__pgprot(	\
+        (((vm_flags) & VM_PKEY_BIT0) >> VM_HIGH_ARCH_BIT_0 ? _PAGE_PKEY_BIT0 : 0) | \
+		(((vm_flags) & VM_PKEY_BIT1) >> VM_HIGH_ARCH_BIT_1 ? _PAGE_PKEY_BIT1 : 0) | \
+		(((vm_flags) & VM_PKEY_BIT2) >> VM_HIGH_ARCH_BIT_2 ? _PAGE_PKEY_BIT2 : 0) | \
+		(((vm_flags) & VM_PKEY_BIT3) >> VM_HIGH_ARCH_BIT_3 ? _PAGE_PKEY_BIT3 : 0) | \
+		(((vm_flags) & VM_PKEY_BIT4) >> VM_HIGH_ARCH_BIT_4 ? _PAGE_PKEY_BIT4 : 0) | \
+		(((vm_flags) & VM_PKEY_BIT5) >> VM_HIGH_ARCH_BIT_5 ? _PAGE_PKEY_BIT5 : 0) | \
+		(((vm_flags) & VM_PKEY_BIT6) >> VM_HIGH_ARCH_BIT_6 ? _PAGE_PKEY_BIT6 : 0) | \
+		(((vm_flags) & VM_PKEY_BIT7) >> VM_HIGH_ARCH_BIT_7 ? _PAGE_PKEY_BIT7 : 0) | \
+		(((vm_flags) & VM_PKEY_BIT8) >> VM_HIGH_ARCH_BIT_8 ? _PAGE_PKEY_BIT8 : 0) | \
+        (((vm_flags) & VM_PKEY_BIT9) >> VM_HIGH_ARCH_BIT_9 ? _PAGE_PKEY_BIT9 : 0))
+#else
 #define arch_vm_get_page_prot(vm_flags) __pgprot(0)
 #endif
+#endif
+/* PHMon: end */
 
 #ifndef arch_validate_prot
 /*
@@ -116,6 +160,11 @@ static inline bool arch_validate_prot(unsigned long prot)
 static inline unsigned long
 calc_vm_prot_bits(unsigned long prot, unsigned long pkey)
 {
+  /* PHMon: begin */
+  #ifdef PHMON_DEBUG
+  printk(" --- call arch_calc_vm_prot_bits from mman.h ---- key: 0x%lx result: 0x%lx\n", pkey, arch_calc_vm_prot_bits(prot, pkey));
+  #endif
+  /* PHMon: end */
 	return _calc_vm_trans(prot, PROT_READ,  VM_READ ) |
 	       _calc_vm_trans(prot, PROT_WRITE, VM_WRITE) |
 	       _calc_vm_trans(prot, PROT_EXEC,  VM_EXEC) |
diff --git a/include/linux/riscv_test_rocc.h b/include/linux/riscv_test_rocc.h
new file mode 100644
index 0000000..e169904
--- /dev/null
+++ b/include/linux/riscv_test_rocc.h
@@ -0,0 +1,16 @@
+// See LICENSE for license details.
+
+#ifndef ROCC_SOFTWARE_SRC_RISCV_TEST_ROCC_H_
+#define ROCC_SOFTWARE_SRC_RISCV_TEST_ROCC_H_
+
+
+#define RVTEST_XS_ENABLE                        \
+  li a0, MSTATUS_XS & (MSTATUS_XS >> 1);        \
+  csrs mstatus, a0;
+
+#define RVTEST_WITH_ROCC                        \
+  .macro init;                                  \
+  RVTEST_XS_ENABLE                              \
+  .endm
+
+#endif  // ROCC_SOFTWARE_SRC_RISCV_TEST_ROCC_H_
diff --git a/include/linux/sched.h b/include/linux/sched.h
index d258826..f2511db 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -101,6 +101,12 @@ struct task_group;
 					 __TASK_TRACED | EXIT_DEAD | EXIT_ZOMBIE | \
 					 TASK_PARKED)
 
+/* PHMon: begin */
+#ifndef CONFIG_KOMODO
+#define CONFIG_KOMODO
+#endif
+/* PHMon: end */
+
 #define task_is_traced(task)		((task->state & __TASK_TRACED) != 0)
 
 #define task_is_stopped(task)		((task->state & __TASK_STOPPED) != 0)
@@ -517,6 +523,54 @@ struct wake_q_node {
 	struct wake_q_node *next;
 };
 
+/* PHMon: begin */
+// Data structure encapsulating everything communicated by one entry
+// in the commit log. This should match the `CommitLog` class from
+// src/main/scala/Varanus.scala.
+struct commit_log_t {
+  unsigned long pc_src;
+  unsigned long pc_dst;
+  unsigned int inst;
+  unsigned int priv;
+  unsigned long rd;
+  unsigned long data;
+};
+
+// Definition of a match on mask that can be used to match a specific
+// commit log. To enable matching of don't cares this requires two
+// masks, "care" and "don't care".
+struct mask_t {
+  struct commit_log_t care;
+  struct commit_log_t dont_care;
+};
+
+//-------------------------------------- Config Unit
+// Data structure encapsulating everything communicated by one entry
+// in the config table. This should match the `ActionConfigTable` class
+// from src/main/scala/ConfigUnit.scala.
+struct act_conf_table_t {
+  unsigned char op_type;
+  unsigned char in1;
+  unsigned char in2;
+  unsigned char fn;
+  unsigned char out;
+  unsigned long data;
+};
+
+struct phmon_info {
+  int index; /* commit index for action */
+  unsigned long action_count; /* number of actions */
+  struct act_conf_table_t action[16]; /* per monitor action list */
+  struct mask_t mu; /* The mu configuration */
+};
+
+#ifdef CONFIG_RISCV_MEMORY_PROTECTION_KEYS
+struct pkru_row_t {
+  u64 row;
+} __packed;
+#endif
+/* PHMon: end */
+
 struct task_struct {
 #ifdef CONFIG_THREAD_INFO_IN_TASK
 	/*
@@ -1097,6 +1151,26 @@ struct task_struct {
 	/* CPU-specific state of this task: */
 	struct thread_struct		thread;
 
+/* PHMon: begin */
+#ifdef CONFIG_RISCV_MEMORY_PROTECTION_KEYS
+    struct pkru_row_t pkru_state[32];
+#endif
+  
+#ifdef CONFIG_KOMODO
+    bool monitor_init; /* indicates whether the monitoring process is initialized for the proc */
+    bool monitor_enable; /* per process monitoring enable flag */
+    bool maintain_state; /* state has been maintened once during context swicht */
+    struct phmon_info info[5]; /* per process monitoring info */
+    unsigned long local1;
+    unsigned long local2;
+    unsigned long local3;
+    unsigned long local4;
+    unsigned long local5;
+    unsigned long local6;
+    struct timespec t0,t1;
+#endif
+/* PHMon: end */
+
 	/*
 	 * WARNING: on x86, 'thread_struct' contains a variable-sized
 	 * structure.  It *MUST* be at the end of 'task_struct'.
diff --git a/include/linux/syscalls.h b/include/linux/syscalls.h
index a78186d..9223392 100644
--- a/include/linux/syscalls.h
+++ b/include/linux/syscalls.h
@@ -940,5 +940,7 @@ asmlinkage long sys_pkey_alloc(unsigned long flags, unsigned long init_val);
 asmlinkage long sys_pkey_free(int pkey);
 asmlinkage long sys_statx(int dfd, const char __user *path, unsigned flags,
 			  unsigned mask, struct statx __user *buffer);
-
+/* PHMon: begin */
+asmlinkage long sys_pkey_seal(int pkey, bool seal_domain, bool seal_page);
+/* PHMon: end   */
 #endif
diff --git a/include/linux/types.h b/include/linux/types.h
index c94d59e..63a98f13 100644
--- a/include/linux/types.h
+++ b/include/linux/types.h
@@ -227,6 +227,27 @@ struct callback_head {
 } __attribute__((aligned(sizeof(void *))));
 #define rcu_head callback_head
 
+/* PHMon: begin */
+#ifdef CONFIG_RISCV_MEMORY_PROTECTION_KEYS
+typedef struct {
+  u32 row[32];
+} pkey_umap_t;
+
+typedef struct {
+  u32 row[32];
+  unsigned long long count[32][32];
+} pkey_dmap_t;
+
+typedef struct {
+  u64 row[32];
+} pkru_t;
+
+typedef struct {
+  u32 row[32];
+} pkey_seal_t;
+#endif
+/* PHMon: end */
+
 typedef void (*rcu_callback_t)(struct rcu_head *head);
 typedef void (*call_rcu_func_t)(struct rcu_head *head, rcu_callback_t func);
 
diff --git a/include/linux/varanus.h b/include/linux/varanus.h
new file mode 100644
index 0000000..b1da0f9
--- /dev/null
+++ b/include/linux/varanus.h
@@ -0,0 +1,238 @@
+#ifndef __VARANUS_H__
+#define __VARANUS_H__
+
+#include <linux/types.h>
+#include "xcustom.h"
+
+/*#ifndef PHMON_DEBUG
+#define PHMON_DEBUG 1
+#endif*/
+
+#define XCUSTOM_KOMODO 1
+// Specific types most closely aligned with their rocket counterparts.
+typedef uint64_t xlen_t; // RISC-V word length: 64 bits
+typedef xlen_t pc_t;     // program counter:    64 bits
+typedef uint32_t inst_t; // instruction:        32 bits
+typedef xlen_t  rd_t;   // rd index:            5 bits
+typedef xlen_t data_t;   // data:               64 bits
+typedef unsigned char op_t; // config table operands  8 bits
+
+// Data structure encapsulating everything communicated by one entry
+// in the commit log. This should match the `CommitLog` class from
+// src/main/scala/Varanus.scala.
+typedef struct {
+  pc_t pc_src;
+  pc_t pc_dst;
+  inst_t inst;
+  inst_t priv;
+  rd_t rd;
+  data_t data;
+} commit_log_t;
+
+// Definition of a match on mask that can be used to match a specific
+// commit log. To enable matching of don't cares this requires two
+// masks, "care" and "don't care".
+typedef struct {
+  commit_log_t care;
+  commit_log_t dont_care;
+} mask_t;
+
+//-------------------------------------- Config Unit
+// Data structure encapsulating everything communicated by one entry
+// in the config table. This should match the `ActionConfigTable` class
+// from src/main/scala/ConfigUnit.scala.
+typedef struct {
+  op_t op_type;
+  op_t in1;
+  op_t in2;
+  op_t fn;
+  op_t out;
+  xlen_t data;
+} act_conf_table_t;
+
+#define read_csr(reg) ({ unsigned long __tmp; \
+  asm volatile ("csrr %0, " #reg : "=r"(__tmp)); \
+  __tmp; })
+
+#define write_csr(reg, val) ({ \
+  asm volatile ("csrw " #reg ", %0" :: "rK"(val)); })
+
+//-------------------------------------- PHMon Monitor
+
+// For a specific comparator number, change its action to the
+// specified config action.
+void komodo_action_config(uint32_t comparator, act_conf_table_t * action);
+
+// For a specific comparator number, change its trigger to the
+// specified mask.
+void komodo_pattern(uint32_t comparator_number, mask_t * mask);
+
+// Enable matching for all comparing units
+#define komodo_reset_all()  \
+  ROCC_INSTRUCTION_0_R_R(XCUSTOM_KOMODO, 0, 0, 2, 11, 12);
+
+// Enable matching for all comparing units
+#define komodo_enable_all()  \
+  ROCC_INSTRUCTION_0_R_R(XCUSTOM_KOMODO, 0, 0, 3, 11, 12);
+  //asm volatile ("custom1 0, 0, 0, 3")
+
+// Disable matching for all comparing units
+#define komodo_disable_all()   \
+  ROCC_INSTRUCTION_0_R_R(XCUSTOM_KOMODO, 0, 0, 4, 11, 12);
+
+// Resume execution after an interrupt is handled
+#define komodo_resume()   \
+  ROCC_INSTRUCTION_0_R_R(XCUSTOM_KOMODO, 0, 0, 15, 11, 12);
+
+// Dequeue one element from activation queue
+#define komodo_dequeu()   \
+  ROCC_INSTRUCTION_0_R_R(XCUSTOM_KOMODO, 0, 0, 10, 11, 12);
+
+#define komodo_done_info(out) \
+  ROCC_INSTRUCTION_R_R_R(XCUSTOM_KOMODO, out, ((uint64_t) e_C_DONE << 32), 0, 1, 10, 11, 12);
+
+// Enable matching for a specific comparator unit (specified with a
+// comparator number).
+void komodo_enable(uint32_t comparator_number);
+
+// Disable matching for a specific comparator unit.
+void komodo_disable(uint32_t comparator_number);
+
+// Reset the match count for a specific comparator unit.
+void komodo_reset_val(uint32_t comparator_number);
+
+// Write the match count
+void komodo_match_count(uint32_t comparator_number, data_t count, xlen_t * addr);
+
+// Write the instruction match count
+void komodo_match_count_instruction(uint32_t comparator_number, data_t count,
+                                    xlen_t * addr);
+
+// Read the value of a specific comparator
+int64_t komodo_read_count(uint32_t comparator_number);
+
+// Read the number of elapsed instructions for a specific comparator
+int64_t komodo_read_instruction_count(uint32_t comparator_number);
+
+void komodo_set_mem_typ(xlen_t type);
+void komodo_set_sp_base(xlen_t *addr);
+void komodo_set_sp_offset(xlen_t *addr);
+void komodo_set_local_reg(uint32_t index, xlen_t *addr);
+
+void komodo_set_commit_index(uint32_t comparator, data_t index);
+
+// Get information about the Komodo monitor
+uint64_t komodo_info(void);
+
+uint64_t komodo_info_sp_base(void);
+uint64_t komodo_info_sp_offset(uint32_t index);
+//uint64_t komodo_done_info(void);
+uint64_t komodo_wait_req(void);
+uint64_t komodo_wait_resp_info(uint64_t action);
+void komodo_mem_cmd(uint64_t  addr, uint64_t data);
+uint64_t komodo_read_mask(uint32_t comparator, uint64_t action);
+uint64_t komodo_read_conf(uint32_t comparator, uint64_t action);
+uint64_t komodo_read_commit_index(uint32_t comparator);
+uint64_t komodo_pkru_rd(uint64_t pkey);
+void komodo_pkru_wr(uint64_t pkey, uint64_t perm);
+
+// Enumerated types for setting a Komodo mask (SM). These should match
+// KomodoParameters in Komodo.scala
+typedef enum {
+  e_SM_PC_SRC_CARE,
+  e_SM_PC_DST_CARE,
+  e_SM_INST_CARE,
+  e_SM_RD_CARE,
+  e_SM_DATA_CARE,
+  e_SM_PC_SRC_DCARE,
+  e_SM_PC_DST_DCARE,
+  e_SM_INST_DCARE,
+  e_SM_RD_DCARE,
+  e_SM_DATA_DCARE
+} komodo_change_mask_e;
+
+// Enumerated types for Komodo control requests (C). These should
+// match KomodoParameters in Komodo.scala
+typedef enum {
+  e_C_VALID,
+  e_C_INVALID,
+  e_C_RESET,
+  e_C_M_COUNT,
+  e_C_LOCAL,
+  e_C_COMMIT_IDX,
+  e_C_INFO_SP_OFFSET,
+  e_C_WRITE_COUNT,
+  e_C_MEM_TYPE,
+  e_C_DONE
+} komodo_ctrl_enum;
+
+typedef enum {
+  e_commit_PC_SRC,
+  e_commit_PC_DST,
+  e_commit_PC_INST,
+  e_commit_PC_DATA,
+  e_commit_PC_ADDR
+} komodo_commit_enum;
+
+typedef enum {
+  e_SC_TYPE,
+  e_SC_IN1,
+  e_SC_IN2,
+  e_SC_FN,
+  e_SC_OUT,
+  e_SC_DATA,
+  e_SC_DONE,
+  e_SC_COUNT
+} komodo_change_conf_e;
+
+typedef enum {
+  e_OP_INTR,
+  e_OP_MEM_WR,
+  e_OP_MEM_RD,
+  e_OP_ALU,
+  e_OP_MEM_XA_ADD
+} komodo_conf_op_e;
+
+typedef enum {
+  e_IN_DATA_MU,
+  e_IN_ADDR_MU,
+  e_IN_CONST,
+  e_IN_LOC1,
+  e_IN_LOC2,
+  e_IN_QUEUE,
+  e_IN_DATA_RESP,
+  e_IN_LOC3,
+  e_IN_LOC4,
+  e_IN_LOC5,
+  e_IN_LOC6,
+  e_IN_COMPRESSED
+} komodo_conf_in_e;
+
+typedef enum {
+  e_OUT_LOC1,
+  e_OUT_LOC2,
+  e_OUT_QUEUE,
+  e_OUT_DATA,
+  e_OUT_ADDR,
+  e_OUT_INTR,
+  e_OUT_LOC3,
+  e_OUT_LOC4,
+  e_OUT_LOC5,
+  e_OUT_LOC6,
+  e_DONE
+} komodo_conf_out_e;
+
+typedef enum {
+  e_ALU_ADD,
+  e_ALU_SUB,
+  e_ALU_SL,
+  e_ALU_SR,
+  e_ALU_SLT,
+  e_ALU_SEQ,
+  e_ALU_AND,
+  e_ALU_OR,
+  e_ALU_XOR,
+  e_ALU_NOP
+} komodo_conf_alu_e;
+
+#endif
diff --git a/include/linux/xcustom.h b/include/linux/xcustom.h
new file mode 100644
index 0000000..0c2beec
--- /dev/null
+++ b/include/linux/xcustom.h
@@ -0,0 +1,83 @@
+// See LICENSE for license details.
+
+#ifndef ROCC_SOFTWARE_SRC_XCUSTOM_H_
+#define ROCC_SOFTWARE_SRC_XCUSTOM_H_
+
+#define STR1(x) #x
+#ifndef STR
+#define STR(x) STR1(x)
+#endif
+#define EXTRACT(a, size, offset) (((~(~0 << size) << offset) & a) >> offset)
+
+// rd = rs2[offset + size - 1 : offset]
+// rs1 is clobbered
+// rs2 is left intact
+#define EXTRACT_RAW(rd, rs1, rs2, size, offset) \
+  not x ## rs1, x0;                             \
+  slli x ## rs1, x ## rs1, size;                \
+  not x ## rs1, x ## rs1;                       \
+  slli x ## rs1, x ## rs1, offset;              \
+  and x ## rd, x ## rs1, x ## rs2;              \
+  srai x ## rd, x ## rd, offset;
+
+#define XCUSTOM_OPCODE(x) XCUSTOM_OPCODE_ ## x
+#define XCUSTOM_OPCODE_0 0b0001011
+#define XCUSTOM_OPCODE_1 0b0101011
+#define XCUSTOM_OPCODE_2 0b1011011
+#define XCUSTOM_OPCODE_3 0b1111011
+
+#define XCUSTOM(x, rd, rs1, rs2, funct)         \
+  XCUSTOM_OPCODE(x)                   |         \
+  (rd                   << (7))       |         \
+  (0x3                  << (7+5))     |         \
+  ((rd != 0) & 1        << (7+5+2))   |         \
+  (rs1                  << (7+5+3))   |         \
+  (rs2                  << (7+5+3+5)) |         \
+  (EXTRACT(funct, 7, 0) << (7+5+3+5+5))
+
+#define ROCC_INSTRUCTION_RAW_R_R_R(x, rd, rs1, rs2, funct)      \
+  .word XCUSTOM(x, ## rd, ## rs1, ## rs2, funct)
+
+// Standard macro that passes rd, rs1, and rs2 via registers
+#define ROCC_INSTRUCTION(x, rd, rs1, rs2, funct)                \
+  ROCC_INSTRUCTION_R_R_R(x, rd, rs1, rs2, funct, 10, 11, 12)
+
+// rd, rs1, and rs2 are data
+// rd_n, rs_1, and rs2_n are the register numbers to use
+#define ROCC_INSTRUCTION_R_R_R(x, rd, rs1, rs2, funct, rd_n, rs1_n, rs2_n) \
+  {                                                                     \
+    register uint64_t rd_  asm ("x" # rd_n);                            \
+    register uint64_t rs1_ asm ("x" # rs1_n) = (uint64_t) rs1;          \
+    register uint64_t rs2_ asm ("x" # rs2_n) = (uint64_t) rs2;          \
+    asm volatile (                                                      \
+        ".word " STR(XCUSTOM(x, rd_n, rs1_n, rs2_n, funct)) "\n\t"      \
+        : "=r" (rd_)                                                    \
+        : [_rs1] "r" (rs1_), [_rs2] "r" (rs2_));                        \
+    rd = rd_;                                                           \
+  }
+
+#define ROCC_INSTRUCTION_0_R_R(x, rs1, rs2, funct, rs1_n, rs2_n)  \
+  {                                                               \
+    register uint64_t rs1_ asm ("x" # rs1_n) = (uint64_t) rs1;    \
+    register uint64_t rs2_ asm ("x" # rs2_n) = (uint64_t) rs2;    \
+    asm volatile (                                                \
+        ".word " STR(XCUSTOM(x, 0, rs1_n, rs2_n, funct)) "\n\t"   \
+        :: [_rs1] "r" (rs1_), [_rs2] "r" (rs2_));                 \
+  }
+
+// [TODO] fix these to align with the above approach
+// Macro to pass rs2_ as an immediate
+/*
+#define ROCC_INSTRUCTION_R_R_I(XCUSTOM_, rd_, rs1_, rs2_, funct_) \
+  asm volatile (XCUSTOM_" %[rd], %[rs1], %[rs2], %[funct]"        \
+                : [rd] "=r" (rd_)                                 \
+                : [rs1] "r" (rs1_), [rs2] "i" (rs2_), [funct] "i" (funct_))
+
+// Macro to pass rs1_ and rs2_ as immediates
+#define ROCC_INSTRUCTION_R_I_I(XCUSTOM_, rd_, rs1_, rs2_, funct_) \
+  asm volatile (XCUSTOM_" %[rd], %[rs1], %[rs2], %[funct]"        \
+                : [rd] "=r" (rd_)                                 \
+                : [rs1] "i" (rs1_), [rs2] "i" (rs2_), [funct] "i" (funct_))
+*/
+
+#endif  // ROCC_SOFTWARE_SRC_XCUSTOM_H_
diff --git a/include/uapi/asm-generic/unistd.h b/include/uapi/asm-generic/unistd.h
index 8b87de0..7879484 100644
--- a/include/uapi/asm-generic/unistd.h
+++ b/include/uapi/asm-generic/unistd.h
@@ -733,8 +733,15 @@ __SYSCALL(__NR_pkey_free,     sys_pkey_free)
 #define __NR_statx 291
 __SYSCALL(__NR_statx,     sys_statx)
 
+/* PHMon: begin */
+#define __NR_pkey_seal 292
+__SYSCALL(__NR_pkey_seal,     sys_pkey_seal)
+
+//#undef __NR_syscalls
+//#define __NR_syscalls 292
 #undef __NR_syscalls
-#define __NR_syscalls 292
+#define __NR_syscalls 293
+/* PHMon: end   */
 
 /*
  * All syscalls below here should go away really,
diff --git a/kernel/exit.c b/kernel/exit.c
index 995453d..68bc70d 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -67,4 +67,7 @@
 #include <asm/unistd.h>
 #include <asm/pgtable.h>
 #include <asm/mmu_context.h>
+/* PHMon: begin */
+#include <linux/varanus.h>
+/* PHMon: end */
 
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -2066,6 +2066,18 @@ long _do_fork(unsigned long clone_flags,
 			get_task_struct(p);
 		}
 
+        /* PHMon: begin */
+        #ifdef CONFIG_RISCV_MEMORY_PROTECTION_KEYS
+        int i;
+        if ((clone_flags & CLONE_VM) && (p->mm != NULL)) {
+          //printk("********** do_fork: clone_vm? :%ld ******** thread[%s] pid: 0x%x\n", clone_flags & CLONE_VM, p->comm, p->pid);
+          for (i = 0; i < PKRU_ROWS; i++) {
+            p->pkru_state[i].row = 0;
+          }
+        }
+        #endif
+        /* PHMon: end */
+
 		wake_up_new_task(p);
 
 		/* forking complete and child started to run, tell ptracer */
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index a7bf32a..cfceec5 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -38,6 +38,14 @@
 #include "sched.h"
 #include "../workqueue_internal.h"
 #include "../smpboot.h"
+/* PHMon: begin */
+#include <linux/varanus.h>
+#include <linux/delay.h>
+#ifdef CONFIG_RISCV_MEMORY_PROTECTION_KEYS
+#include <asm/pgtable.h>
+#include <linux/pkeys.h>
+#endif
+/* PHMon: end */
 
 #define CREATE_TRACE_POINTS
 #include <trace/events/sched.h>
@@ -2761,6 +2769,29 @@ context_switch(struct rq *rq, struct task_struct *prev,
 {
 	struct mm_struct *mm, *oldmm;
 
+    /* PHMon: begin */
+    int i , j;
+    
+    int count = 0;
+    
+    /* Store pkru status */
+    #ifdef CONFIG_RISCV_MEMORY_PROTECTION_KEYS
+    struct pt_regs *regs;
+	regs = task_pt_regs(prev);
+    //if (user_mode(regs) && mm_pkey_allocated(prev->active_mm)) {
+    if (user_mode(regs) && (prev->mm != NULL)) {
+      #ifdef PHMON_DEBUG
+      printk("context switch from prev thread[%s] pid: 0x%x tgid: 0x%x\n", prev->comm, prev->pid, prev->tgid);
+      #endif
+      for (i = 0; i < PKRU_ROWS; i++) {
+        prev->pkru_state[i].row = komodo_pkru_rd(i*32);
+        #ifdef PHMON_DEBUG
+        printk("prev: pkru_state[%ld] = 0x%llx\n", i, prev->pkru_state[i].row);
+        #endif
+      }
+    }
+    #endif
+    /* PHMon: end */
 	prepare_task_switch(rq, prev, next);
 
 	mm = next->mm;
@@ -2795,11 +2872,32 @@ context_switch(struct rq *rq, struct task_struct *prev,
 	rq_unpin_lock(rq, rf);
 	spin_release(&rq->lock.dep_map, 1, _THIS_IP_);
 
+
+    /* PHMon: begin */
+    /* Restore pkru status */
+    #ifdef CONFIG_RISCV_MEMORY_PROTECTION_KEYS
+	regs = task_pt_regs(current);
+    if (user_mode(regs) && (next->mm != NULL)) {
+    //if (user_mode(regs) && mm_pkey_allocated(current->active_mm)) {
+      #ifdef PHMON_DEBUG
+      printk("context switch to next thread[%s] pid: 0x%x tgid: 0x%x\n", next->comm, next->pid, next->tgid);
+      #endif
+      for (i = 0; i < PKRU_ROWS; i++) {
+        #ifdef PHMON_DEBUG
+        printk("next: pkru[0x%x] = 0x%llx\n", i * 32, next->pkru_state[i].row);
+        #endif
+        komodo_pkru_wr(i * 32, next->pkru_state[i].row);
+      }
+    }
+    #endif
+    
+    /* PHMon: end */
+    
 	/* Here we just switch the register state and the stack. */
 	switch_to(prev, next, prev);
 	barrier();
-
-	return finish_task_switch(prev);
+    
+    return finish_task_switch(prev);
 }
 
 /*
diff --git a/mm/memory.c b/mm/memory.c
index 7930046..647edca 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -4114,6 +4114,11 @@ int handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
 		if (task_in_memcg_oom(current) && !(ret & VM_FAULT_OOM))
 			mem_cgroup_oom_synchronize(false);
 	}
+    /* PHMon: begin */
+    #ifdef PHMON_DEBUG
+    printk("handle_mm_fault (memory.c): ret? %d\n", ret);
+    #endif
+    /* PHMon: end */
 
 	return ret;
 }
diff --git a/mm/mmap.c b/mm/mmap.c
index 9efdc021..3a5928d 100644
--- a/mm/mmap.c
+++ b/mm/mmap.c
@@ -51,6 +51,12 @@
 #include <asm/tlb.h>
 #include <asm/mmu_context.h>
 
+/* PHMon: begin */
+#ifdef CONFIG_RISCV_MEMORY_PROTECTION_KEYS
+#include <linux/varanus.h>
+#endif
+/* PHMon: end */
+
 #include "internal.h"
 
 #ifndef arch_mmap_check
@@ -102,6 +108,12 @@ pgprot_t protection_map[16] __ro_after_init = {
 
 pgprot_t vm_get_page_prot(unsigned long vm_flags)
 {
+  /* PHMon: begin */
+  #ifdef PHMON_DEBUG
+  printk("---- vm_get_page_prot(mmap.c) ---- vm_flags: 0x%lx get_page_prot: 0x%lx pgprot_val(arch_vm_get_page_prot(vm_flags): 0x%lx\n", vm_flags, arch_vm_get_page_prot(vm_flags), pgprot_val(arch_vm_get_page_prot(vm_flags)));
+  //printk("---- vm_get_page_prot(mmap.c) ---- vm_flags: 0x%lx VM_PKEY_BIT0: 0x%lx and: 0x%lx shift: 0x%llx result: 0x%llx \n", vm_flags, VM_PKEY_BIT0, ((vm_flags) & VM_PKEY_BIT0) >> VM_HIGH_ARCH_BIT_0, (1UL << _PAGE_BIT_PKEY_BIT0), _PAGE_PKEY_BIT0);
+  #endif
+  /* PHMon: end */
 	return __pgprot(pgprot_val(protection_map[vm_flags &
 				(VM_READ|VM_WRITE|VM_EXEC|VM_SHARED)]) |
 			pgprot_val(arch_vm_get_page_prot(vm_flags)));
@@ -124,6 +136,11 @@ void vma_set_page_prot(struct vm_area_struct *vma)
 		vm_flags &= ~VM_SHARED;
 		vm_page_prot = vm_pgprot_modify(vm_page_prot, vm_flags);
 	}
+    /* PHMon: begin */
+    #ifdef PHMON_DEBUG
+    printk("---- vma_set_page_prot (mmap.c) ---- vm_page_prot: 0x%lx\n", vm_page_prot);
+    #endif
+    /* PHMon: end */
 	/* remove_protection_ptes reads vma->vm_page_prot without mmap_sem */
 	WRITE_ONCE(vma->vm_page_prot, vm_page_prot);
 }
diff --git a/mm/mprotect.c b/mm/mprotect.c
index 58b629b..816d442 100644
--- a/mm/mprotect.c
+++ b/mm/mprotect.c
@@ -34,6 +34,12 @@
 
 #include "internal.h"
 
+/* PHMon: begin */
+#ifdef CONFIG_RISCV_MEMORY_PROTECTION_KEYS
+#include <linux/varanus.h>
+#endif
+/* PHMon: end */
+
 static unsigned long change_pte_range(struct vm_area_struct *vma, pmd_t *pmd,
 		unsigned long addr, unsigned long end, pgprot_t newprot,
 		int dirty_accountable, int prot_numa)
@@ -43,6 +49,12 @@ static unsigned long change_pte_range(struct vm_area_struct *vma, pmd_t *pmd,
 	spinlock_t *ptl;
 	unsigned long pages = 0;
 	int target_node = NUMA_NO_NODE;
+    /* PHMon: begin */
+    /*int oldpkey;
+    int newpkey;
+    int upperpkey;
+    int lowerpkey;*/
+    /* PHMon: end */
 
 	/*
 	 * Can be called with only the mmap_sem for reading by
@@ -69,6 +81,9 @@ static unsigned long change_pte_range(struct vm_area_struct *vma, pmd_t *pmd,
 	arch_enter_lazy_mmu_mode();
 	do {
 		oldpte = *pte;
+        #ifdef PHMON_DEBUG
+        //printk("--- change_pte_range (mprotect.c) ---- old pte: 0x%lx addr: 0x%llx end: 0x%llx\n", pte_val(oldpte), addr, end);
+        #endif
 		if (pte_present(oldpte)) {
 			pte_t ptent;
 			bool preserve_write = prot_numa && pte_write(oldpte);
@@ -98,6 +113,28 @@ static unsigned long change_pte_range(struct vm_area_struct *vma, pmd_t *pmd,
 
 			ptent = ptep_modify_prot_start(mm, addr, pte);
 			ptent = pte_modify(ptent, newprot);
+            /* PHMon: begin */
+            /*oldpkey = pte_val(oldpte) >> _PAGE_PD_SHIFT;
+            lowerpkey = oldpkey & ((1UL << 5) - 1);
+            upperpkey = oldpkey >> 5;
+            if (mm->context.pkey_dirty_map.count[upperpkey][lowerpkey]) {
+              mm->context.pkey_dirty_map.count[upperpkey][lowerpkey]--;
+              }*/
+
+            #ifdef PHMON_DEBUG
+            printk("--- change_pte_range (mprotect.c) ---- old pte: 0x%lx old pkey: 0x%x upper bits: 0x%x lower bits: 0x%x count: 0x%llx\n", pte_val(oldpte), oldpkey, upperpkey, lowerpkey, mm_pkey_dirty_map_count(mm, upperpkey, lowerpkey));
+            #endif
+            
+            /*newpkey = pte_val(ptent) >> _PAGE_PD_SHIFT;
+            lowerpkey = newpkey & ((1UL << 5) - 1);
+            upperpkey = newpkey >> 5;
+            mm->context.pkey_dirty_map.count[upperpkey][lowerpkey]++;*/
+            
+            #ifdef PHMON_DEBUG
+            printk("--- change_pte_range (mprotect.c) ---- newprot: 0x%lx pte: 0x%lx\n", pgprot_val(newprot), pte_val(ptent));
+            printk("--- change_pte_range (mprotect.c) ---- new pte: 0x%lx new pkey: 0x%x upper bits: 0x%x lower bits: 0x%x count: 0x%llx\n", pte_val(ptent), newpkey, upperpkey, lowerpkey, mm_pkey_dirty_map_count(mm, upperpkey, lowerpkey));
+            #endif
+            /* PHMon: end */
 			if (preserve_write)
 				ptent = pte_mk_savedwrite(ptent);
 
@@ -400,6 +437,43 @@ static int do_mprotect_pkey(unsigned long start, size_t len,
 	const bool rier = (current->personality & READ_IMPLIES_EXEC) &&
 				(prot & PROT_READ);
 
+    /* PHMon: begin */
+    int old_pkey;
+    
+    #ifdef PHMON_DEBUG
+    printk("start of mprotect\n");
+    #endif
+
+    xlen_t mask;
+    struct page *page;
+    struct mm_struct *mm;
+    pgd_t *pgd;
+    p4d_t *p4d;
+    pud_t *pud;
+    pmd_t *pmd;
+    pte_t *pte;
+    unsigned long addr = start;
+
+    int old_row, row, old_column, column;;
+    bool page_sealed, domain_sealed, old_page_sealed;
+
+    #ifdef PHMON_DEBUG
+    page = NULL;
+    mm = current->mm;
+    //mask out bits 39-63
+    mask = (1UL << 39)-1;
+    addr &= mask;
+
+    pgd = pgd_offset(mm, addr);
+    p4d = p4d_offset(pgd, addr);
+    pud = pud_offset(p4d, addr);
+    pmd = pmd_offset(pud, addr);
+    pte = pte_offset_map(pmd, addr);
+    
+    printk("pte = 0x%llx, pte_page = 0x%llx\n", pte->pte, pte_page(*pte));
+    #endif
+    /* PHMon: end */
+
 	prot &= ~(PROT_GROWSDOWN|PROT_GROWSUP);
 	if (grows == (PROT_GROWSDOWN|PROT_GROWSUP)) /* can't be both */
 		return -EINVAL;
@@ -416,6 +490,11 @@ static int do_mprotect_pkey(unsigned long start, size_t len,
 		return -EINVAL;
 
 	reqprot = prot;
+    /* PHMon: begin */
+    #ifdef PHMON_DEBUG
+    printk("reqprot: 0x%lx\n", prot);
+    #endif
+    /* PHMon: end */
 
 	if (down_write_killable(&current->mm->mmap_sem))
 		return -EINTR;
@@ -427,12 +506,40 @@ static int do_mprotect_pkey(unsigned long start, size_t len,
 	error = -EINVAL;
 	if ((pkey != -1) && !mm_pkey_is_allocated(current->mm, pkey))
 		goto out;
+    /* PHMon: begin */
+    #ifdef PHMON_DEBUG
+    printk("userspace allocated pkey\n");
+    #endif
+    /* PHMon: end */
 
 	vma = find_vma(current->mm, start);
 	error = -ENOMEM;
 	if (!vma)
 		goto out;
 	prev = vma->vm_prev;
+    /* PHMon: begin */
+    old_pkey = vma_pkey(vma);
+    old_row = old_pkey >> 5;
+    old_column = (old_pkey << 5) >> 5;
+    row = pkey >> 5;
+    column = (pkey << 5) >> 5;
+
+    domain_sealed = ((1UL << old_column) & current->mm->context.pkey_domain_seal_map.row[old_row]) >> old_column;
+    page_sealed = ((1UL << column) & current->mm->context.pkey_page_seal_map.row[row]) >> column;
+    old_page_sealed = ((1UL << old_column) & current->mm->context.pkey_page_seal_map.row[old_row]) >> old_column;;
+
+    if (domain_sealed && (old_pkey != 0) && (old_pkey != pkey))
+    {
+      //printk("------- mprotect: ------- row: 0x%x cloumn; 0x%x domain_row: 0x%x result: 0x%lx\n", old_row, old_column, current->mm->context.pkey_domain_seal_map.row[old_row], ((1UL << old_column) & current->mm->context.pkey_domain_seal_map.row[old_row]) >> old_column);
+      goto out;
+    }
+
+    if (page_sealed || (old_page_sealed && (old_pkey == pkey)))
+    {
+      //printk("------- mprotect: ------- row: 0x%x cloumn; 0x%x domain_row: 0x%x result: 0x%lx\n", row, column, current->mm->context.pkey_domain_seal_map.row[row], ((1UL << column) & current->mm->context.pkey_domain_seal_map.row[row]) >> column);
+      goto out;
+    }
+    /* PHMon: end   */
 	if (unlikely(grows & PROT_GROWSDOWN)) {
 		if (vma->vm_start >= end)
 			goto out;
@@ -473,8 +580,29 @@ static int do_mprotect_pkey(unsigned long start, size_t len,
 					ARCH_VM_PKEY_FLAGS;
 
 		new_vma_pkey = arch_override_mprotect_pkey(vma, prot, pkey);
+        /* PHMon: begin */
+        #ifdef PHMON_DEBUG
+        printk("new_vma_pkey: 0x%x\n", new_vma_pkey);
+        #endif
+        /* PHMon: end */
 		newflags = calc_vm_prot_bits(prot, new_vma_pkey);
+        /* PHMon: begin */
+        #ifdef PHMON_DEBUG
+        printk("new flags from calc_vm_prot_bits: 0x%lx\n", newflags);
+        #endif
+        /* PHMon: end */
 		newflags |= (vma->vm_flags & ~mask_off_old_flags);
+        /* PHMon: begin */
+        #ifdef PHMON_DEBUG
+        printk("vma flags: 0x%lx mask_off_old_flags: 0x%lx new flags: 0x%lx\n", vma->vm_flags, mask_off_old_flags, newflags);
+        #endif
+        /* PHMon: end */
+
+        /* PHMon: begin */
+        #ifdef PHMON_DEBUG
+        printk("new flags: 0x%lx ~(newflags >> 4): 0x%lx (VM_READ|VM_WRITE|VM_EXEC): 0x%x result: 0x%lx\n", newflags, ~(newflags >> 4), (VM_READ | VM_WRITE | VM_EXEC), (newflags & ~(newflags >> 4)) & (VM_READ | VM_WRITE | VM_EXEC));
+        #endif
+        /* PHMon: end */
 
 		/* newflags >> 4 shift VM_MAY% in place of VM_% */
 		if ((newflags & ~(newflags >> 4)) & (VM_READ | VM_WRITE | VM_EXEC)) {
@@ -483,6 +611,11 @@ static int do_mprotect_pkey(unsigned long start, size_t len,
 		}
 
 		error = security_file_mprotect(vma, reqprot, prot);
+        /* PHMon: begin */
+        #ifdef PHMON_DEBUG
+        printk("security_file_mprotect: error? %d\n", error);
+        #endif
+        /* PHMon: end */
 		if (error)
 			goto out;
 
@@ -505,8 +638,34 @@ static int do_mprotect_pkey(unsigned long start, size_t len,
 			goto out;
 		}
 		prot = reqprot;
+        /* PHMon: begin */
+        #ifdef PHMON_DEBUG
+        printk("----- mprotect ------ prot: 0x%lx\n", prot);
+        #endif
+        /* PHMon: end */
 	}
 out:
+    /* PHMon: begin */
+    #ifdef PHMON_DEBUG
+    printk("---- mprotect ----- error: %d\n", error);
+    #endif
+
+    #ifdef PHMON_DEBUG
+    page = NULL;
+    mm = current->mm;
+    //mask out bits 39-63
+    mask = (1UL << 39)-1;
+    addr &= mask;
+
+    pgd = pgd_offset(mm, addr);
+    p4d = p4d_offset(pgd, addr);
+    pud = pud_offset(p4d, addr);
+    pmd = pmd_offset(pud, addr);
+    pte = pte_offset_map(pmd, addr);
+    printk("pte = 0x%llx, pte_page = 0x%llx\n", pte->pte, pte_page(*pte));
+    #endif
+    /* PHMon: end */
+    
 	up_write(&current->mm->mmap_sem);
 	return error;
 }
@@ -530,6 +689,11 @@ SYSCALL_DEFINE2(pkey_alloc, unsigned long, flags, unsigned long, init_val)
 	int pkey;
 	int ret;
 
+    /* PHMon: begin */
+    #ifdef PHMON_DEBUG
+    printk("enter pkey_alloc\n");
+    #endif
+    /* PHMon: end */
 	/* No flags supported yet. */
 	if (flags)
 		return -EINVAL;
@@ -539,6 +703,11 @@ SYSCALL_DEFINE2(pkey_alloc, unsigned long, flags, unsigned long, init_val)
 
 	down_write(&current->mm->mmap_sem);
 	pkey = mm_pkey_alloc(current->mm);
+    /* PHMon: begin */
+    #ifdef PHMON_DEBUG
+    printk("pkey: 0x%x %d\n", pkey, pkey);
+    #endif
+    /* PHMon: end */
 
 	ret = -ENOSPC;
 	if (pkey == -1)
@@ -570,4 +739,33 @@ SYSCALL_DEFINE1(pkey_free, int, pkey)
 	return ret;
 }
 
+/* PHMon: begin */
+SYSCALL_DEFINE3(pkey_seal, int, pkey, bool, seal_domain, bool, seal_page)
+{
+	int domain_seal_row;
+    int page_seal_row;
+    int ret;
+
+    int row = pkey >> 5;
+    int column = (pkey << 5) >> 5;
+
+    if (seal_domain == 1)
+    {
+      domain_seal_row = current->mm->context.pkey_domain_seal_map.row[row];
+      current->mm->context.pkey_domain_seal_map.row[row] = (1UL << column) | domain_seal_row;
+      //printk("------------- pkey_seal --- seal domain; domain_seall_row: 0x%x -------------", current->mm->context.pkey_domain_seal_map.row[row]);
+    }
+
+    if (seal_page == 1)
+    {
+      page_seal_row = current->mm->context.pkey_page_seal_map.row[row];
+      current->mm->context.pkey_page_seal_map.row[row] = (1UL << column) | page_seal_row;
+      //printk("------------- pkey_seal --- seal page; page_seall_row: 0x%x -------------", current->mm->context.pkey_page_seal_map.row[row]);
+      }
+
+    ret = 1;
+    return ret;
+}
+/* PHMon: end */
+
 #endif /* CONFIG_ARCH_HAS_PKEYS */
diff --git a/stOt1IIm b/stOt1IIm
new file mode 100644
index 0000000..69343eb
--- /dev/null
+++ b/stOt1IIm
@@ -0,0 +1 @@
+!<thin>
